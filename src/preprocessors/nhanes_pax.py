"""
NHANES PAX-Gãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†

CDC FTPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰1ãƒ¦ãƒ¼ã‚¶ãƒ¼ãšã¤ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å‡¦ç†ã—ã€
å‡¦ç†å¾Œã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ç¯€ç´„ã—ã¾ã™ã€‚
"""

import tarfile
import tempfile
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
import urllib.request
from urllib.error import URLError, HTTPError
import time
import logging

import numpy as np
import pandas as pd
from scipy import signal
from tqdm import tqdm

from .base import BasePreprocessor
from . import register_preprocessor

logger = logging.getLogger(__name__)


class NHANESPAXProcessor:
    """NHANES PAX-Gãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        output_base: str = "/mnt/home/processed_data/NHANES_PAX",
        window_size: int = 5,  # seconds
        sampling_rate: int = 80,  # Hz
        target_sampling_rate: Optional[int] = None,  # Hz (Noneã®å ´åˆã¯ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãªã„)
        std_threshold: float = 0.02,  # æ¨™æº–åå·®ã®åˆè¨ˆã®é–¾å€¤
        temp_dir: Optional[str] = None,
        max_retries: int = 3,
        retry_delay: int = 5
    ):
        """
        Args:
            output_base: å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            window_size: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆç§’ï¼‰
            sampling_rate: å…ƒã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆHzï¼‰- NHANES PAXã¯80Hz
            target_sampling_rate: ç›®æ¨™ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆHzï¼‰- Noneã®å ´åˆã¯ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãªã„
            std_threshold: ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£åˆ¤å®šç”¨ã®æ¨™æº–åå·®é–¾å€¤
            temp_dir: ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆNoneã®å ´åˆã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            max_retries: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            retry_delay: ãƒªãƒˆãƒ©ã‚¤é–“éš”ï¼ˆç§’ï¼‰
        """
        self.output_base = Path(output_base)
        self.window_size = window_size
        self.sampling_rate = sampling_rate
        self.target_sampling_rate = target_sampling_rate if target_sampling_rate is not None else sampling_rate
        self.std_threshold = std_threshold
        self.temp_dir = temp_dir
        self.max_retries = max_retries
        self.retry_delay = retry_delay

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é•·ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ï¼‰- ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œã®ãƒ¬ãƒ¼ãƒˆã§è¨ˆç®—
        self.window_length = window_size * self.target_sampling_rate
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.output_base.mkdir(parents=True, exist_ok=True)
        
        # FTPã‚µãƒ¼ãƒãƒ¼ã®ãƒ™ãƒ¼ã‚¹URLï¼ˆPAX-Gã¨PAX-Hã®2ã¤ã®ã‚½ãƒ¼ã‚¹ï¼‰
        self.ftp_base_g = "https://ftp.cdc.gov/pub/pax_g/"
        self.ftp_base_h = "https://ftp.cdc.gov/pub/pax_h/"
        self.pax_h_start_id = 73557  # PAX-Hã®é–‹å§‹ID
        
        # é€²æ—ç®¡ç†ãƒ•ã‚¡ã‚¤ãƒ«
        self.progress_file = self.output_base / "processing_progress.json"
        self.failed_users_file = self.output_base / "failed_users.json"
    
    def check_user_exists(self, user_id: int) -> bool:
        """
        FTPã‚µãƒ¼ãƒãƒ¼ã§ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            
        Returns:
            ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹
        """
        import urllib.request
        from urllib.error import URLError, HTTPError
        
        # PAX-Gã¨PAX-Hã®ã©ã¡ã‚‰ã‹ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if user_id < self.pax_h_start_id:
            url = f"{self.ftp_base_g}{user_id}.tar.bz2"
        else:
            url = f"{self.ftp_base_h}{user_id}.tar.bz2"
            
        try:
            # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãªã„ï¼‰
            req = urllib.request.Request(url)
            req.get_method = lambda: 'HEAD'
            response = urllib.request.urlopen(req, timeout=10)
            return response.status == 200
        except HTTPError as e:
            return e.code == 200
        except (URLError, Exception):
            return False
    
    def discover_available_users(self, start_id: int = 62161, end_id: int = 72161, 
                               batch_size: int = 100) -> List[int]:
        """
        åˆ©ç”¨å¯èƒ½ãªãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’è‡ªå‹•ç™ºè¦‹
        
        Args:
            start_id: é–‹å§‹ID
            end_id: çµ‚äº†ID
            batch_size: ä¸€åº¦ã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹IDæ•°
            
        Returns:
            å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
        """
        print(f"Discovering available users from {start_id} to {end_id}...")
        
        available_users = []
        total_range = end_id - start_id + 1
        
        # ãƒãƒƒãƒã”ã¨ã«ç¢ºèª
        for batch_start in tqdm(range(start_id, end_id + 1, batch_size), 
                               desc="Checking user availability"):
            batch_end = min(batch_start + batch_size - 1, end_id)
            batch_ids = list(range(batch_start, batch_end + 1))
            
            # ä¸¦åˆ—ã§ãƒã‚§ãƒƒã‚¯
            from multiprocessing import Pool
            with Pool(processes=4) as pool:
                results = pool.map(self.check_user_exists, batch_ids)
            
            # å­˜åœ¨ã™ã‚‹IDã‚’è¿½åŠ 
            for user_id, exists in zip(batch_ids, results):
                if exists:
                    available_users.append(user_id)
        
        print(f"Found {len(available_users)} available users out of {total_range} checked")
        if available_users:
            print(f"Available user IDs: {sorted(available_users)}")
        return available_users
    
    def get_user_ids(self, start_id: int = 62161, end_id: int = 72161, 
                    discover: bool = False) -> List[int]:
        """
        å‡¦ç†å¯¾è±¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            start_id: é–‹å§‹ID
            end_id: çµ‚äº†ID
            discover: å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ã¿ã‚’å–å¾—ã™ã‚‹ã‹
            
        Returns:
            ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
        """
        if discover:
            return self.discover_available_users(start_id, end_id)
        else:
            return list(range(start_id, end_id + 1))
    
    def save_progress(self, processed_users: List[int], failed_users: List[int], 
                     current_batch: int = 0, total_batches: int = 0) -> None:
        """
        é€²æ—çŠ¶æ³ã‚’ä¿å­˜
        
        Args:
            processed_users: å‡¦ç†æ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
            failed_users: å¤±æ•—ã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
            current_batch: ç¾åœ¨ã®ãƒãƒƒãƒç•ªå·
            total_batches: ç·ãƒãƒƒãƒæ•°
        """
        import json
        from datetime import datetime
        
        progress_data = {
            "last_updated": datetime.now().isoformat(),
            "processed_users": processed_users,
            "failed_users": failed_users,
            "current_batch": current_batch,
            "total_batches": total_batches,
            "total_processed": len(processed_users),
            "total_failed": len(failed_users)
        }
        
        with open(self.progress_file, 'w') as f:
            json.dump(progress_data, f, indent=2)
    
    def load_progress(self) -> Tuple[List[int], List[int], int, int]:
        """
        é€²æ—çŠ¶æ³ã‚’èª­ã¿è¾¼ã¿
        
        Returns:
            (processed_users, failed_users, current_batch, total_batches)
        """
        import json
        
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r') as f:
                    data = json.load(f)
                return (
                    data.get("processed_users", []),
                    data.get("failed_users", []),
                    data.get("current_batch", 0),
                    data.get("total_batches", 0)
                )
            except Exception as e:
                print(f"Warning: Could not load progress file: {e}")
        
        return [], [], 0, 0
    
    def get_remaining_users(self, all_users: List[int]) -> List[int]:
        """
        å‡¦ç†ãŒå¿…è¦ãªæ®‹ã‚Šã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—
        
        Args:
            all_users: å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
            
        Returns:
            å‡¦ç†ãŒå¿…è¦ãªãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
        """
        processed_users, failed_users, _, _ = self.load_progress()
        completed_users = set(processed_users + failed_users)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚‚ç¢ºèª
        remaining_users = []
        for user_id in all_users:
            if user_id not in completed_users:
                output_path = self.output_base / "nhanes" / f"USER{user_id}" / "PAX" / "ACC" / "X.npy"
                if not output_path.exists():
                    remaining_users.append(user_id)
                else:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ãŒé€²æ—ã«è¨˜éŒ²ã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¨˜éŒ²
                    processed_users.append(user_id)
        
        # é€²æ—ã‚’æ›´æ–°
        if len(processed_users) != len(set(processed_users)):
            processed_users = list(set(processed_users))
            self.save_progress(processed_users, failed_users)
        
        return remaining_users
    
    def download_user_data(self, user_id: int, temp_path: Path) -> bool:
        """
        ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            temp_path: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆã®ä¸€æ™‚ãƒ‘ã‚¹
            
        Returns:
            ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸã®å¯å¦
        """
        # PAX-Gã¨PAX-Hã®ã©ã¡ã‚‰ã‹ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if user_id < self.pax_h_start_id:
            url = f"{self.ftp_base_g}{user_id}.tar.bz2"
        else:
            url = f"{self.ftp_base_h}{user_id}.tar.bz2"
            
        download_path = temp_path / f"{user_id}.tar.bz2"
        
        for attempt in range(self.max_retries):
            try:
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²è¡ŒçŠ¶æ³ç”¨ã®pbar
                pbar = None
                
                def show_progress(block_num, block_size, total_size):
                    nonlocal pbar
                    if pbar is None:
                        pbar = tqdm(
                            total=total_size,
                            unit='B',
                            unit_scale=True,
                            desc=f"Downloading user {user_id}"
                        )
                    downloaded = block_num * block_size
                    if downloaded < total_size:
                        pbar.update(block_size)
                    else:
                        pbar.update(total_size - pbar.n)
                        pbar.close()
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆé€²è¡ŒçŠ¶æ³è¡¨ç¤ºä»˜ãï¼‰
                urllib.request.urlretrieve(url, download_path, reporthook=show_progress)
                
                if pbar and not pbar.disable:
                    pbar.close()
                    
                return True
                
            except HTTPError as e:
                if pbar and not pbar.disable:
                    pbar.close()

                # 404ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å³åº§ã«å¤±æ•—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ï¼‰
                if e.code == 404:
                    return False

                # ãã®ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒˆãƒ©ã‚¤
                if attempt < self.max_retries - 1:
                    print(f"Download failed for user {user_id}, attempt {attempt + 1}/{self.max_retries}: {e}")
                    time.sleep(self.retry_delay)
                else:
                    print(f"Failed to download user {user_id} after {self.max_retries} attempts: {e}")
                    return False
            except URLError as e:
                if pbar and not pbar.disable:
                    pbar.close()

                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒˆãƒ©ã‚¤
                if attempt < self.max_retries - 1:
                    print(f"Download failed for user {user_id}, attempt {attempt + 1}/{self.max_retries}: {e}")
                    time.sleep(self.retry_delay)
                else:
                    print(f"Failed to download user {user_id} after {self.max_retries} attempts: {e}")
                    return False
            except Exception as e:
                if pbar and not pbar.disable:
                    pbar.close()
                    
                print(f"Unexpected error downloading user {user_id}: {e}")
                return False
        
        return False
    
    def extract_archive(self, archive_path: Path, extract_path: Path) -> bool:
        """
        tar.bz2ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’è§£å‡
        
        Args:
            archive_path: ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            extract_path: è§£å‡å…ˆã®ãƒ‘ã‚¹
            
        Returns:
            è§£å‡æˆåŠŸã®å¯å¦
        """
        try:
            with tarfile.open(archive_path, 'r:bz2') as tar:
                tar.extractall(extract_path)
            return True
        except Exception as e:
            print(f"Failed to extract {archive_path}: {e}")
            return False
    
    def process_sensor_file(self, file_path: Path) -> Optional[np.ndarray]:
        """
        å€‹åˆ¥ã®ã‚»ãƒ³ã‚µãƒ¼CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å«ã‚€ï¼‰

        Args:
            file_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
            å‡¦ç†ã•ã‚ŒãŸã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆ3ãƒãƒ£ãƒ³ãƒãƒ«Ã—ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼‰
        """
        try:
            # CSVã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv(
                file_path,
                names=["timestamp", "X", "Y", "Z"],
                skiprows=1,
                on_bad_lines='skip',
                dtype={"timestamp": str, "X": float, "Y": float, "Z": float}
            )

            # æ•°å€¤ã«å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼ã¯NaNã«ï¼‰
            x = pd.to_numeric(df["X"], errors='coerce').values
            y = pd.to_numeric(df["Y"], errors='coerce').values
            z = pd.to_numeric(df["Z"], errors='coerce').values

            # NaNã‚’é™¤å¤–
            valid_mask = ~(np.isnan(x) | np.isnan(y) | np.isnan(z))
            x = x[valid_mask]
            y = y[valid_mask]
            z = z[valid_mask]

            if len(x) == 0:
                return None

            # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¿…è¦ãªå ´åˆï¼‰
            if self.target_sampling_rate != self.sampling_rate:
                # ãƒãƒªãƒ•ã‚§ãƒ¼ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨åŒã˜æ–¹æ³•ï¼‰
                from math import gcd

                # æ¯”ç‡ã‚’ç°¡ç´„åŒ–ï¼ˆä¾‹: 80Hz -> 30Hz = up=3, down=8ï¼‰
                multiplier = 1000
                up = int(self.target_sampling_rate * multiplier)
                down = int(self.sampling_rate * multiplier)
                common_divisor = gcd(up, down)
                up = up // common_divisor
                down = down // common_divisor

                # å„ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ãƒãƒªãƒ•ã‚§ãƒ¼ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                x = signal.resample_poly(x, up, down)
                y = signal.resample_poly(y, up, down)
                z = signal.resample_poly(z, up, down)

            # (3, samples) ã®å½¢çŠ¶ã§è¿”ã™ã€float16ã§åŠ¹ç‡åŒ–
            result = np.stack([x, y, z], axis=0).astype(np.float16)
            return result

        except Exception as e:
            print(f"Error processing sensor file {file_path}: {e}")
            return None
    
    def extract_valid_windows(self, data: np.ndarray) -> Optional[np.ndarray]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–¾å€¤ä»¥ä¸Šã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãŒã‚ã‚‹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æŠ½å‡º
        
        Args:
            data: ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆ3ãƒãƒ£ãƒ³ãƒãƒ«Ã—ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼‰
            
        Returns:
            æœ‰åŠ¹ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°Ã—3ãƒãƒ£ãƒ³ãƒãƒ«Ã—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é•·ï¼‰
        """
        total_length = data.shape[1]
        n_windows = total_length // self.window_length
        
        if n_windows == 0:
            return None
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«åˆ†å‰²
        segments = data[:, :n_windows * self.window_length].reshape(
            3, n_windows, self.window_length
        )
        
        # å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®æ¨™æº–åå·®ã‚’è¨ˆç®—
        stds = np.std(segments, axis=2)  # shape: (3, n_windows)
        std_sum = np.sum(stds, axis=0)   # shape: (n_windows,)
        
        # é–¾å€¤ä»¥ä¸Šã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æŠ½å‡º
        valid_mask = std_sum >= self.std_threshold
        valid_segments = segments[:, valid_mask, :]  # shape: (3, n_valid, window_length)
        
        if valid_segments.shape[1] > 0:
            # (n_valid, 3, window_length) ã®å½¢çŠ¶ã«å¤‰æ›
            return valid_segments.transpose(1, 0, 2)
        else:
            return None
    
    def process_user(self, user_id: int) -> Tuple[bool, str]:
        """
        1ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰â†’å‡¦ç†â†’ä¿å­˜â†’å‰Šé™¤ï¼‰
        
        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            
        Returns:
            (æˆåŠŸãƒ•ãƒ©ã‚°, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
        """
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆX.npyã§ç¢ºèªï¼‰
        output_dir = self.output_base / "nhanes" / f"USER{user_id}" / "PAX" / "ACC"
        data_path = output_dir / "X.npy"
        if data_path.exists():
            return True, f"User {user_id} already processed, skipping"
        
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        with tempfile.TemporaryDirectory(dir=self.temp_dir) as temp_dir:
            temp_path = Path(temp_dir)
            
            # 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if not self.download_user_data(user_id, temp_path):
                return False, f"Failed to download user {user_id}"
            
            # 2. è§£å‡
            archive_path = temp_path / f"{user_id}.tar.bz2"
            extract_path = temp_path / "extracted"
            extract_path.mkdir(exist_ok=True)
            
            if not self.extract_archive(archive_path, extract_path):
                return False, f"Failed to extract user {user_id}"
            
            # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤
            archive_path.unlink()
            
            # 3. ã‚»ãƒ³ã‚µãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            sensor_files = list(extract_path.glob("**/*.sensor.csv"))
            if not sensor_files:
                return False, f"No sensor files found for user {user_id}"
            
            all_segments = []
            for sensor_file in sensor_files:
                # ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                data = self.process_sensor_file(sensor_file)
                if data is None:
                    continue
                
                # æœ‰åŠ¹ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æŠ½å‡º
                valid_windows = self.extract_valid_windows(data)
                if valid_windows is not None:
                    all_segments.append(valid_windows)
            
            # 4. ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            if all_segments:
                # ã™ã¹ã¦ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµåˆ
                user_data = np.concatenate(all_segments, axis=0)

                # float16ã§ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
                user_data = user_data.astype(np.float16)

                # ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆï¼ˆå…¨ã¦-1ã€float16ï¼‰
                labels = np.full(user_data.shape[0], -1, dtype=np.float16)

                # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
                output_dir.mkdir(parents=True, exist_ok=True)

                # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                np.save(data_path, user_data)
                np.save(output_dir / "Y.npy", labels)

                return True, f"Successfully processed user {user_id}: {user_data.shape[0]} windows saved"
            else:
                return False, f"No valid windows found for user {user_id}"
    
    def process_all_users(
        self,
        user_ids: Optional[List[int]] = None,
        start_id: int = 62161,
        end_id: int = 72161,
        parallel: bool = False,
        n_workers: int = 4
    ):
        """
        ã™ã¹ã¦ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å‡¦ç†ï¼ˆé€²æ—ç®¡ç†ä»˜ãï¼‰
        
        Args:
            user_ids: å‡¦ç†ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯ç¯„å›²æŒ‡å®šï¼‰
            start_id: é–‹å§‹IDï¼ˆuser_idsãŒNoneã®å ´åˆï¼‰
            end_id: çµ‚äº†IDï¼ˆuser_idsãŒNoneã®å ´åˆï¼‰
            parallel: ä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            n_workers: ä¸¦åˆ—å‡¦ç†æ™‚ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒªã‚¹ãƒˆã‚’æº–å‚™
        if user_ids is None:
            user_ids = self.get_user_ids(start_id, end_id)
        
        # é€²æ—çŠ¶æ³ã‚’èª­ã¿è¾¼ã¿
        processed_users, failed_users, last_batch, total_batches = self.load_progress()
        
        # æ®‹ã‚Šã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—
        remaining_users = self.get_remaining_users(user_ids)
        
        print(f"Total users: {len(user_ids)}")
        print(f"Already processed: {len(processed_users)}")
        print(f"Previously failed: {len(failed_users)}")
        print(f"Remaining to process: {len(remaining_users)}")
        
        if not remaining_users:
            print("All users have been processed!")
            return
        
        print(f"Resuming processing from user batch...")
        
        if parallel:
            # ä¸¦åˆ—å‡¦ç†ï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ï¼‰
            from multiprocessing import Pool
            
            with Pool(processes=n_workers) as pool:
                results = list(tqdm(
                    pool.imap_unordered(self.process_user, remaining_users),
                    total=len(remaining_users),
                    desc="Processing remaining users"
                ))
        else:
            # é€æ¬¡å‡¦ç†ï¼ˆé€²æ—ä¿å­˜ä»˜ãï¼‰
            results = []
            for i, user_id in enumerate(tqdm(remaining_users, desc="Processing remaining users")):
                result = self.process_user(user_id)
                results.append(result)
                
                success, message = result
                if success and "already processed" not in message:
                    processed_users.append(user_id)
                    print(f"âœ“ {message}")
                else:
                    failed_users.append(user_id)
                    print(f"âš ï¸  {message}")
                
                # å®šæœŸçš„ã«é€²æ—ã‚’ä¿å­˜ï¼ˆ10ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ï¼‰
                if (i + 1) % 10 == 0:
                    self.save_progress(processed_users, failed_users)
                    print(f"Progress saved: {len(processed_users)} processed, {len(failed_users)} failed")
        
        # æœ€çµ‚çš„ãªé€²æ—ã‚’ä¿å­˜
        if not parallel:
            self.save_progress(processed_users, failed_users)
        
        # çµ±è¨ˆã‚’è¡¨ç¤º
        successful = sum(1 for success, _ in results if success)
        failed = len(results) - successful
        
        print(f"\n" + "="*60)
        print(f"Processing complete!")
        print(f"âœ“ Successful this session: {successful}/{len(results)}")
        print(f"âœ— Failed this session: {failed}/{len(results)}")
        print(f"ğŸ“Š Total processed: {len(processed_users)}/{len(user_ids)}")
        print(f"ğŸ“Š Total failed: {len(failed_users)}/{len(user_ids)}")
        print("="*60)


@register_preprocessor('nhanes')
class NHANESPreprocessor(BasePreprocessor):
    """NHANES PAX-Gãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹ï¼ˆçµ±åˆç‰ˆï¼‰"""
    
    def get_dataset_name(self) -> str:
        return "nhanes"
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

        # NHANESå›ºæœ‰ã®è¨­å®šï¼ˆPAX-Gã¨PAX-Hã®2ã¤ã®ã‚½ãƒ¼ã‚¹ï¼‰
        self.ftp_base_g = "https://ftp.cdc.gov/pub/pax_g/"
        self.ftp_base_h = "https://ftp.cdc.gov/pub/pax_h/"
        self.pax_h_start_id = 73557  # PAX-Hã®é–‹å§‹ID
        self.window_size = config.get('window_size', 5)  # seconds
        self.sampling_rate = config.get('sampling_rate', 80)  # Hz (å…ƒã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ)
        self.target_sampling_rate = config.get('target_sampling_rate', 30)  # Hz (ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å…ˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30Hz)
        self.std_threshold = config.get('std_threshold', 0.02)
        self.start_id = config.get('start_id', 62161)
        self.end_id = config.get('end_id', 62170)
        self.batch_size = config.get('batch_size', 100)
        self.max_retries = config.get('max_retries', 3)
        self.retry_delay = config.get('retry_delay', 5)
        self.parallel = config.get('parallel', True)
        self.n_workers = config.get('workers', 4)

        # æœ€çµ‚çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        final_rate = self.target_sampling_rate if self.target_sampling_rate is not None else self.sampling_rate

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é•·ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ï¼‰- ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œã®ãƒ¬ãƒ¼ãƒˆã§è¨ˆç®—
        self.window_length = self.window_size * final_rate

        # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’åˆæœŸåŒ–
        self.processor = NHANESPAXProcessor(
            output_base=str(self.processed_data_path),
            window_size=self.window_size,
            sampling_rate=self.sampling_rate,
            target_sampling_rate=self.target_sampling_rate,
            std_threshold=self.std_threshold,
            max_retries=self.max_retries,
            retry_delay=self.retry_delay
        )
    
    def download_dataset(self) -> None:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        NHANESã®å ´åˆã¯å€‹åˆ¥ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å‡¦ç†ã™ã‚‹ãŸã‚ã€
        ã“ã“ã§ã¯ä½•ã‚‚ã—ãªã„ï¼ˆprocess_all_usersã§å®Ÿè¡Œï¼‰
        """
        logger.info("NHANES PAX-G dataset will be downloaded automatically during processing")
    
    def load_raw_data(self) -> List[int]:
        """
        å‡¦ç†å¯¾è±¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒªã‚¹ãƒˆã‚’è¿”ã™
        
        Returns:
            ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
        """
        # è¨­å®šã§ç™ºè¦‹ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        discover_mode = self.config.get('discover_users', False)
        
        if discover_mode:
            logger.info("User discovery mode enabled. Scanning for available users...")
            user_ids = self.processor.discover_available_users(
                start_id=self.start_id,
                end_id=self.end_id,
                batch_size=self.config.get('discovery_batch_size', 100)
            )
            logger.info(f"Discovered {len(user_ids)} available users: {sorted(user_ids)}")
        else:
            user_ids = list(range(self.start_id, self.end_id + 1))
            logger.info(f"Using ID range mode: {len(user_ids)} users ({self.start_id} to {self.end_id})")
        
        return user_ids
    
    def clean_data(self, user_ids: List[int]) -> List[int]:
        """
        ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ—¢ã«å‡¦ç†æ¸ˆã¿ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’é™¤å¤–ï¼‰
        
        Args:
            user_ids: å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
            
        Returns:
            å‡¦ç†å¯¾è±¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
        """
        # é€²æ—ç®¡ç†æ©Ÿèƒ½ã‚’ä½¿ç”¨
        remaining_users = self.processor.get_remaining_users(user_ids)
        
        already_processed = len(user_ids) - len(remaining_users)
        if already_processed > 0:
            logger.info(f"Skipping {already_processed} already processed users")
        
        logger.info(f"Remaining users to process: {len(remaining_users)}")
        return remaining_users
    
    def extract_features(self, user_ids: List[int]) -> Dict[str, Any]:
        """
        ç‰¹å¾´æŠ½å‡ºï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ï¼‰
        
        Args:
            user_ids: å‡¦ç†å¯¾è±¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã®ãƒªã‚¹ãƒˆ
            
        Returns:
            å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
        """
        if not user_ids:
            logger.info("No users to process")
            return {"processed_users": 0, "total_users": 0}
        
        # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†
        total_users = len(user_ids)
        processed_users = 0
        failed_users = 0
        
        for i in range(0, len(user_ids), self.batch_size):
            batch_users = user_ids[i:i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            total_batches = (len(user_ids) + self.batch_size - 1) // self.batch_size
            
            logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch_users)} users)")
            
            # ãƒãƒƒãƒå‡¦ç†
            if self.parallel:
                from multiprocessing import Pool
                
                with Pool(processes=self.n_workers) as pool:
                    results = list(tqdm(
                        pool.imap_unordered(self.processor.process_user, batch_users),
                        total=len(batch_users),
                        desc=f"Batch {batch_num}"
                    ))
            else:
                results = []
                for user_id in tqdm(batch_users, desc=f"Batch {batch_num}"):
                    results.append(self.processor.process_user(user_id))
            
            # çµæœã‚’é›†è¨ˆ
            batch_success = sum(1 for success, _ in results if success)
            batch_failed = len(results) - batch_success
            
            processed_users += batch_success
            failed_users += batch_failed
            
            logger.info(f"Batch {batch_num} complete: {batch_success}/{len(batch_users)} succeeded")
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¿”ã™
        return {
            "total_users": total_users,
            "processed_users": processed_users,
            "failed_users": failed_users,
            "success_rate": processed_users / total_users if total_users > 0 else 0
        }
    
    def save_processed_data(self, stats: Dict[str, Any]) -> None:
        """
        å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆçµ±è¨ˆæƒ…å ±ã®ä¿å­˜ï¼‰
        
        Args:
            stats: å‡¦ç†çµæœã®çµ±è¨ˆæƒ…å ±
        """
        # çµ±è¨ˆæƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        import json
        
        stats_file = self.processed_data_path / "processing_stats.json"
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)
        
        logger.info(f"Processing statistics saved to {stats_file}")
    
    def preprocess(self) -> None:
        """
        å‰å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†
        """
        logger.info("Starting NHANES PAX-G preprocessing...")
        
        # 1. å‡¦ç†å¯¾è±¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒªã‚¹ãƒˆã‚’å–å¾—
        user_ids = self.load_raw_data()
        
        # 2. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ—¢ã«å‡¦ç†æ¸ˆã¿ã‚’é™¤å¤–ï¼‰
        remaining_users = self.clean_data(user_ids)
        
        # 3. ç‰¹å¾´æŠ½å‡ºï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰â†’å‡¦ç†â†’ä¿å­˜ï¼‰
        stats = self.extract_features(remaining_users)
        
        # 4. çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
        self.save_processed_data(stats)
        
        logger.info("NHANES PAX-G preprocessing completed!")
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        å‡¦ç†çµ±è¨ˆã‚’å–å¾—
        
        Returns:
            çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        stats_file = self.processed_data_path / "processing_stats.json"
        
        if stats_file.exists():
            import json
            with open(stats_file, 'r') as f:
                return json.load(f)
        else:
            # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            processed_count = len(list(self.processed_data_path.glob("nhanes/USER*/PAX/ACC/X.npy")))
            return {
                "processed_users": processed_count,
                "total_users": self.end_id - self.start_id + 1
            }