# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¿½åŠ ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€har-unified-datasetã«æ–°ã—ã„HARãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã™ã‚‹éš›ã®æ‰‹é †ã¨å¿ƒå¾—ã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚

**å¯¾è±¡**: äººé–“ã®é–‹ç™ºè€…ã€AIï¼ˆClaude Codeç­‰ï¼‰

**é‡è¦**: AIãŒã“ã®ã‚¬ã‚¤ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’é †ç•ªã«å®Ÿè¡Œã—ã€æ—¢å­˜ã®å®Ÿè£…ï¼ˆdsads.pyã€mhealth.pyï¼‰ã‚’**å¿…ãšå‚ç…§**ã—ã¦ã‹ã‚‰å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚æ¨æ¸¬ã‚„å‰µä½œã¯é¿ã‘ã€**ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¿ å®Ÿ**ã«å¾“ã£ã¦ãã ã•ã„ã€‚

---

## ğŸš¨ æœ€é‡è¦ï¼šã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¿½åŠ æ™‚ã¯å¿…ãš[CRITICAL_CHECKLIST.md](CRITICAL_CHECKLIST.md)ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚**

éå»ã«ç™ºç”Ÿã—ãŸé‡å¤§ãªãƒã‚°ï¼ˆãƒ©ãƒ™ãƒ«é †åºã®é–“é•ã„ã€ã‚¹ã‚±ãƒ¼ãƒ«ã®ç•°å¸¸ã€ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦‹è½ã¨ã—ï¼‰ã‚’é˜²ããŸã‚ã®å¿…é ˆç¢ºèªäº‹é …ãŒã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

---

## ç›®æ¬¡

1. [äº‹å‰æº–å‚™ï¼šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç†è§£](#äº‹å‰æº–å‚™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç†è§£)
2. [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¿½åŠ ã®5ã‚¹ãƒ†ãƒƒãƒ—](#ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¿½åŠ ã®5ã‚¹ãƒ†ãƒƒãƒ—)
3. [å®Ÿè£…ã®è©³ç´°](#å®Ÿè£…ã®è©³ç´°)
4. [é‡è¦ãªè¨­è¨ˆåŸå‰‡](#é‡è¦ãªè¨­è¨ˆåŸå‰‡)
5. [ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ](#ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ)
6. [AIå®Ÿè£…æ™‚ã®æ³¨æ„äº‹é …](#aiå®Ÿè£…æ™‚ã®æ³¨æ„äº‹é …)

---

## äº‹å‰æº–å‚™ï¼šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç†è§£

æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã™ã‚‹å‰ã«ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’å®Œå…¨ã«ç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š

### å¿…é ˆæƒ…å ±

- **ãƒ‡ãƒ¼ã‚¿æ§‹é€ **
  - ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ï¼ˆCSV, TXT, MAT, NPYãªã©ï¼‰
  - ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆï¼ˆè¢«é¨“è€…åˆ¥ã€æ´»å‹•åˆ¥ãªã©ï¼‰
  - ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å½¢æ…‹ï¼ˆé€£ç¶šãƒ‡ãƒ¼ã‚¿ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå˜ä½ãªã©ï¼‰

- **ã‚»ãƒ³ã‚µãƒ¼æƒ…å ±**
  - ã‚»ãƒ³ã‚µãƒ¼æ•°ã¨è£…ç€ä½ç½®ï¼ˆä¾‹ï¼šèƒ¸éƒ¨ã€æ‰‹é¦–ã€è¶³é¦–ï¼‰
  - ã‚»ãƒ³ã‚µãƒ¼ã‚¿ã‚¤ãƒ—ã¨ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼ˆACC: 3è»¸ã€GYRO: 3è»¸ã€ECG: 2ãƒãƒ£ãƒ³ãƒãƒ«ãªã©ï¼‰
  - å„ã‚»ãƒ³ã‚µãƒ¼ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£æ§‹æˆ

- **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æƒ…å ±**
  - ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆHzï¼‰
  - ã‚µãƒ³ãƒ—ãƒ«æ•°ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·
  - æ™‚é–“çš„ãªé€£ç¶šæ€§ã®æœ‰ç„¡

- **ãƒ©ãƒ™ãƒ«æƒ…å ±**
  - æ´»å‹•ã‚¯ãƒ©ã‚¹æ•°
  - ãƒ©ãƒ™ãƒ«ã®å½¢å¼ï¼ˆæ•°å€¤ã€æ–‡å­—åˆ—ã€éšå±¤çš„ãªã©ï¼‰
  - æœªå®šç¾©ã‚¯ãƒ©ã‚¹ï¼ˆ-1ã‚„Nullãªã©ï¼‰ã®æœ‰ç„¡
  - ãƒ©ãƒ™ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ–¹å¼ï¼ˆ0-indexedã€1-indexedï¼‰

- **ãƒ‡ãƒ¼ã‚¿å˜ä½ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**
  - **æœ€é‡è¦**: åŠ é€Ÿåº¦ã‚»ãƒ³ã‚µãƒ¼ã®å˜ä½ï¼ˆGã€m/sÂ²ã€mg ãªã©ï¼‰
  - ã‚¸ãƒ£ã‚¤ãƒ­ã®å˜ä½ï¼ˆrad/sã€deg/s ãªã©ï¼‰
  - æ­£è¦åŒ–ã‚„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒæ—¢ã«é©ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹

- **è¢«é¨“è€…æƒ…å ±**
  - è¢«é¨“è€…æ•°
  - IDå½¢å¼ï¼ˆé€£ç•ªã€æ–‡å­—åˆ—ãªã©ï¼‰

---

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¿½åŠ ã®5ã‚¹ãƒ†ãƒƒãƒ—

### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç™»éŒ²

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/dataset_info.py`

`DATASETS`è¾æ›¸ã«æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’è¿½åŠ ã—ã¾ã™ã€‚

```python
DATASETS = {
    # ...æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ...

    "YOUR_DATASET": {
        "sensor_list": ["Sensor1", "Sensor2"],  # ã‚»ãƒ³ã‚µãƒ¼ä½ç½®ã®ãƒªã‚¹ãƒˆ
        "modalities": ["ACC", "GYRO"],          # å…¨ã‚»ãƒ³ã‚µãƒ¼å…±é€šã®å ´åˆ
        # ã¾ãŸã¯
        "modalities": {                          # ã‚»ãƒ³ã‚µãƒ¼ã”ã¨ã«ç•°ãªã‚‹å ´åˆ
            "Sensor1": ["ACC", "GYRO"],
            "Sensor2": ["ACC", "ECG"]
        },
        "n_classes": 10,                         # æœ‰åŠ¹ãªã‚¯ãƒ©ã‚¹æ•°
        "sampling_rate": 30,                     # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œã®ãƒ¬ãƒ¼ãƒˆ
        "original_sampling_rate": 50,            # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ¬ãƒ¼ãƒˆ
        "scale_factor": 9.8,                     # åŠ é€Ÿåº¦ã®å˜ä½å¤‰æ›ä¿‚æ•°ï¼ˆm/sÂ² â†’ Gï¼‰
        "has_undefined_class": True,             # -1ãƒ©ãƒ™ãƒ«ã®æœ‰ç„¡
        "labels": {
            -1: 'Undefined',  # has_undefined_class=Trueã®å ´åˆã®ã¿
            0: 'Activity1',
            1: 'Activity2',
            # ...
        },
    },
}
```

#### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

1. **`scale_factor`ã®æ±ºå®š**ï¼ˆæœ€é‡è¦ï¼‰
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠ é€Ÿåº¦ãŒ**m/sÂ²å˜ä½**ã®å ´åˆï¼š`scale_factor: 9.8`
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠ é€Ÿåº¦ãŒ**Gå˜ä½**ã®å ´åˆï¼š`scale_factor`ã‚’çœç•¥ã¾ãŸã¯None
   - ã“ã®ä¿‚æ•°ã¯**ACCãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿**ã«é©ç”¨ã•ã‚Œã‚‹ï¼ˆGYROã€MAGã«ã¯é©ç”¨ã•ã‚Œãªã„ï¼‰
   - ç›®çš„ï¼šç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§ã®åŠ é€Ÿåº¦ã‚¹ã‚±ãƒ¼ãƒ«ã‚’**Gå˜ä½ã«çµ±ä¸€**

2. **`modalities`ã®æŒ‡å®šæ–¹æ³•**
   - å…¨ã‚»ãƒ³ã‚µãƒ¼ãŒåŒã˜ãƒ¢ãƒ€ãƒªãƒ†ã‚£æ§‹æˆï¼šãƒªã‚¹ãƒˆå½¢å¼
   - ã‚»ãƒ³ã‚µãƒ¼ã”ã¨ã«ç•°ãªã‚‹ï¼šè¾æ›¸å½¢å¼ï¼ˆMHEALTHã®ä¾‹ã‚’å‚ç…§ï¼‰

3. **`has_undefined_class`ãƒ•ãƒ©ã‚°**
   - ãƒ©ãƒ™ãƒ«-1ï¼ˆæœªå®šç¾©/ç„¡æ´»å‹•ï¼‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼š`True`
   - ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ãŒæœ‰åŠ¹ãªã‚¯ãƒ©ã‚¹ï¼š`False`

---

### ã‚¹ãƒ†ãƒƒãƒ—2: å‰å‡¦ç†ã‚¯ãƒ©ã‚¹ã®å®Ÿè£…

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/preprocessors/your_dataset.py`

`BasePreprocessor`ã‚’ç¶™æ‰¿ã—ãŸæ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚

```python
"""
YOUR_DATASET (Your Dataset Name) å‰å‡¦ç†

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦:
- Nç¨®é¡ã®æ´»å‹•
- Mäººã®è¢«é¨“è€…
- Kå€‹ã®ã‚»ãƒ³ã‚µãƒ¼
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: XXHz
"""

import numpy as np
from pathlib import Path
from typing import Dict, Any, Tuple
import logging

from .base import BasePreprocessor
from .utils import (
    create_sliding_windows,
    filter_invalid_samples,
    resample_timeseries,
    get_class_distribution
)
from .common import (
    download_file,
    extract_archive,
    cleanup_temp_files,
    check_dataset_exists
)
from . import register_preprocessor
from ..dataset_info import DATASETS

logger = logging.getLogger(__name__)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®URLï¼ˆå…¬é–‹ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
YOUR_DATASET_URL = "https://example.com/dataset.zip"


@register_preprocessor('your_dataset')
class YourDatasetPreprocessor(BasePreprocessor):
    """
    YOUR_DATASETãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®å‰å‡¦ç†ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®è¨­å®š
        self.num_activities = 10
        self.num_subjects = 20
        self.num_sensors = 3

        # ã‚»ãƒ³ã‚µãƒ¼ã¨ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        self.sensor_names = ['Sensor1', 'Sensor2', 'Sensor3']
        self.sensor_channel_ranges = {
            'Sensor1': (0, 6),   # channels 0-5
            'Sensor2': (6, 12),  # channels 6-11
            'Sensor3': (12, 18)  # channels 12-17
        }

        # ãƒ¢ãƒ€ãƒªãƒ†ã‚£ï¼ˆå„ã‚»ãƒ³ã‚µãƒ¼å†…ã®ãƒãƒ£ãƒ³ãƒãƒ«åˆ†å‰²ï¼‰
        self.sensor_modalities = {
            'Sensor1': {
                'ACC': (0, 3),   # 3è»¸åŠ é€Ÿåº¦
                'GYRO': (3, 6),  # 3è»¸ã‚¸ãƒ£ã‚¤ãƒ­
            },
            # ...ä»–ã®ã‚»ãƒ³ã‚µãƒ¼...
        }

        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        self.original_sampling_rate = 50  # Hz (ã‚ªãƒªã‚¸ãƒŠãƒ«)
        self.target_sampling_rate = config.get('target_sampling_rate', 30)  # Hz (ç›®æ¨™)

        # å‰å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.window_size = config.get('window_size', 150)  # 5ç§’ @ 30Hz
        self.stride = config.get('stride', 30)  # 1ç§’ @ 30Hz

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆdataset_info.pyã‹ã‚‰å–å¾—ï¼‰
        self.scale_factor = DATASETS.get('YOUR_DATASET', {}).get('scale_factor', None)

    def get_dataset_name(self) -> str:
        return 'your_dataset'

    def download_dataset(self) -> None:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£å‡

        æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªå ´åˆã¯ NotImplementedError ã‚’ç™ºç”Ÿã•ã›ã‚‹
        """
        # å®Ÿè£…ä¾‹ã¯dsads.pyã‚’å‚ç…§

    def load_raw_data(self) -> Dict[int, Tuple[np.ndarray, np.ndarray]]:
        """
        ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¢«é¨“è€…ã”ã¨ã«èª­ã¿è¾¼ã‚€

        Returns:
            {person_id: (data, labels)} ã®è¾æ›¸
                data: (num_samples, num_channels) ã®é…åˆ—
                labels: (num_samples,) ã®é…åˆ—
        """
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®èª­ã¿è¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
        pass

    def clean_data(self, data: Dict[int, Tuple[np.ndarray, np.ndarray]]) -> Dict[int, Tuple[np.ndarray, np.ndarray]]:
        """
        ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        """
        cleaned = {}
        for person_id, (person_data, labels) in data.items():
            # ç„¡åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ã‚’é™¤å»
            cleaned_data, cleaned_labels = filter_invalid_samples(person_data, labels)

            # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¿…è¦ãªå ´åˆï¼‰
            if self.original_sampling_rate != self.target_sampling_rate:
                resampled_data, resampled_labels = resample_timeseries(
                    cleaned_data,
                    cleaned_labels,
                    self.original_sampling_rate,
                    self.target_sampling_rate
                )
                cleaned[person_id] = (resampled_data, resampled_labels)
                logger.info(f"USER{person_id:05d} cleaned and resampled: {resampled_data.shape}")
            else:
                cleaned[person_id] = (cleaned_data, cleaned_labels)
                logger.info(f"USER{person_id:05d} cleaned: {cleaned_data.shape}")

        return cleaned

    def extract_features(self, data: Dict[int, Tuple[np.ndarray, np.ndarray]]) -> Dict[int, Dict[str, Dict[str, np.ndarray]]]:
        """
        ç‰¹å¾´æŠ½å‡ºï¼ˆã‚»ãƒ³ã‚µãƒ¼Ã—ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åŒ–ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰

        Returns:
            {person_id: {sensor/modality: {'X': data, 'Y': labels}}}
        """
        processed = {}

        for person_id, (person_data, labels) in data.items():
            logger.info(f"Processing USER{person_id:05d}")
            processed[person_id] = {}

            # å„ã‚»ãƒ³ã‚µãƒ¼ã«ã¤ã„ã¦å‡¦ç†
            for sensor_name in self.sensor_names:
                sensor_start_ch, sensor_end_ch = self.sensor_channel_ranges[sensor_name]
                sensor_data = person_data[:, sensor_start_ch:sensor_end_ch]

                # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é©ç”¨
                windowed_data, windowed_labels = create_sliding_windows(
                    sensor_data,
                    labels,
                    window_size=self.window_size,
                    stride=self.stride,
                    drop_last=False,
                    pad_last=True
                )

                # å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«åˆ†å‰²
                modalities = self.sensor_modalities[sensor_name]
                for modality_name, (mod_start_ch, mod_end_ch) in modalities.items():
                    modality_data = windowed_data[:, :, mod_start_ch:mod_end_ch]

                    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨ï¼ˆACCã®ã¿ã€scale_factorãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                    if modality_name == 'ACC' and self.scale_factor is not None:
                        modality_data = modality_data / self.scale_factor
                        logger.info(f"  Applied scale_factor={self.scale_factor} to {sensor_name}/{modality_name}")

                    # å½¢çŠ¶å¤‰æ›: (N, T, C) -> (N, C, T)
                    modality_data = np.transpose(modality_data, (0, 2, 1))

                    # float16ã«å¤‰æ›ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
                    modality_data = modality_data.astype(np.float16)

                    sensor_modality_key = f"{sensor_name}/{modality_name}"
                    processed[person_id][sensor_modality_key] = {
                        'X': modality_data,
                        'Y': windowed_labels
                    }

                    logger.info(
                        f"  {sensor_modality_key}: X.shape={modality_data.shape}, "
                        f"Y.shape={windowed_labels.shape}"
                    )

        return processed

    def save_processed_data(self, data: Dict[int, Dict[str, Dict[str, np.ndarray]]]) -> None:
        """
        å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜

        ä¿å­˜å½¢å¼:
            data/processed/your_dataset/USER00001/Sensor1/ACC/X.npy, Y.npy
        """
        import json

        base_path = self.processed_data_path / self.dataset_name
        base_path.mkdir(parents=True, exist_ok=True)

        total_stats = {
            'dataset': self.dataset_name,
            'num_activities': self.num_activities,
            'num_sensors': self.num_sensors,
            'sensor_names': self.sensor_names,
            'original_sampling_rate': self.original_sampling_rate,
            'target_sampling_rate': self.target_sampling_rate,
            'window_size': self.window_size,
            'stride': self.stride,
            'normalization': 'none',
            'scale_factor': self.scale_factor,
            'data_dtype': 'float16',
            'users': {}
        }

        for person_id, sensor_modality_data in data.items():
            user_name = f"USER{person_id:05d}"
            user_path = base_path / user_name
            user_path.mkdir(parents=True, exist_ok=True)

            user_stats = {'sensor_modalities': {}}

            for sensor_modality_name, arrays in sensor_modality_data.items():
                sensor_modality_path = user_path / sensor_modality_name
                sensor_modality_path.mkdir(parents=True, exist_ok=True)

                X = arrays['X']
                Y = arrays['Y']

                np.save(sensor_modality_path / 'X.npy', X)
                np.save(sensor_modality_path / 'Y.npy', Y)

                user_stats['sensor_modalities'][sensor_modality_name] = {
                    'X_shape': X.shape,
                    'Y_shape': Y.shape,
                    'num_windows': len(Y),
                    'class_distribution': get_class_distribution(Y)
                }

                logger.info(f"Saved {user_name}/{sensor_modality_name}: X{X.shape}, Y{Y.shape}")

            total_stats['users'][user_name] = user_stats

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆNumPyå‹ã‚’JSONäº’æ›ã«å¤‰æ›ï¼‰
        metadata_path = base_path / 'metadata.json'
        with open(metadata_path, 'w') as f:
            def convert_to_serializable(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, tuple):
                    return list(obj)
                return obj

            def recursive_convert(d):
                if isinstance(d, dict):
                    return {k: recursive_convert(v) for k, v in d.items()}
                elif isinstance(d, list):
                    return [recursive_convert(v) for v in d]
                else:
                    return convert_to_serializable(d)

            serializable_stats = recursive_convert(total_stats)
            json.dump(serializable_stats, f, indent=2)

        logger.info(f"Saved metadata to {metadata_path}")
        logger.info(f"Preprocessing completed: {base_path}")
```

---

### ã‚¹ãƒ†ãƒƒãƒ—3: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `configs/preprocess.yaml`

```yaml
datasets:
  # ...æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ...

  your_dataset:
    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
    raw_data_path: data/raw
    processed_data_path: data/processed

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
    target_sampling_rate: 30  # Hz (å…ƒã¯XXHzã‹ã‚‰ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)

    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    window_size: 150      # 5ç§’ @ 30Hz
    stride: 30            # 1ç§’ @ 30Hz (80%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ï¼ˆå‚è€ƒç”¨ï¼‰
    num_activities: 10
    num_subjects: 20
    num_channels: 18
    original_sampling_rate: 50  # Hz (å…ƒãƒ‡ãƒ¼ã‚¿)
```

---

### ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ†ã‚¹ãƒˆã®ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/test_your_dataset.py`

```python
"""
YOUR_DATASETå‰å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ
"""

import pytest
import numpy as np
from pathlib import Path

from src.preprocessors.your_dataset import YourDatasetPreprocessor


@pytest.fixture
def config():
    """ãƒ†ã‚¹ãƒˆç”¨ã®è¨­å®š"""
    return {
        'raw_data_path': 'data/raw',
        'processed_data_path': 'data/processed',
        'target_sampling_rate': 30,
        'window_size': 150,
        'stride': 30,
    }


def test_preprocessor_initialization(config):
    """å‰å‡¦ç†ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
    preprocessor = YourDatasetPreprocessor(config)
    assert preprocessor.dataset_name == 'your_dataset'
    assert preprocessor.target_sampling_rate == 30


def test_scale_factor_loading(config):
    """scale_factorãŒdataset_info.pyã‹ã‚‰æ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã‚‹ã‹"""
    preprocessor = YourDatasetPreprocessor(config)
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒm/sÂ²å˜ä½ã®å ´åˆ
    assert preprocessor.scale_factor == 9.8  # or None
```

---

### ã‚¹ãƒ†ãƒƒãƒ—5: å‹•ä½œç¢ºèª

```bash
# 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
python preprocess.py --list

# 2. å‰å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰+å‡¦ç†ï¼‰
python preprocess.py --dataset your_dataset --download

# 3. å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
ls -R data/processed/your_dataset/

# 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
cat data/processed/your_dataset/metadata.json

# 5. ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
pytest tests/test_your_dataset.py -v
```

---

## å®Ÿè£…ã®è©³ç´°

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ç†è§£

```
ç”Ÿãƒ‡ãƒ¼ã‚¿
  â†“ load_raw_data()
{person_id: (data, labels)}  # data: (samples, channels), labels: (samples,)
  â†“ clean_data()
{person_id: (cleaned_data, cleaned_labels)}  # ç„¡åŠ¹ã‚µãƒ³ãƒ—ãƒ«é™¤å»ã€ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
  â†“ extract_features()
{person_id: {sensor/modality: {'X': windowed_data, 'Y': windowed_labels}}}
  # X: (num_windows, channels, window_size) - float16
  # Y: (num_windows,) - int
  â†“ save_processed_data()
ãƒ‡ã‚£ã‚¹ã‚¯ä¿å­˜: data/processed/dataset/USER00001/Sensor/Modality/X.npy, Y.npy
```

### é‡è¦ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—

#### 1. ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆclean_dataå†…ï¼‰

```python
# ãƒãƒªãƒ•ã‚§ãƒ¼ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹é«˜å“è³ªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
resampled_data, resampled_labels = resample_timeseries(
    cleaned_data,
    cleaned_labels,
    self.original_sampling_rate,  # å…ƒã®ãƒ¬ãƒ¼ãƒˆ
    self.target_sampling_rate      # ç›®æ¨™ãƒ¬ãƒ¼ãƒˆï¼ˆé€šå¸¸30Hzï¼‰
)
```

- ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’**30Hz**ã«çµ±ä¸€
- ã‚¢ãƒ³ãƒã‚¨ã‚¤ãƒªã‚¢ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
- ãƒ©ãƒ™ãƒ«ã¯æœ€è¿‘å‚è£œé–“

#### 2. ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åŒ–ï¼ˆextract_featureså†…ï¼‰

```python
windowed_data, windowed_labels = create_sliding_windows(
    sensor_data,           # (samples, channels)
    labels,               # (samples,)
    window_size=150,      # 5ç§’ @ 30Hz
    stride=30,            # 1ç§’ @ 30Hz (80%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—)
    drop_last=False,      # æœ€å¾Œã®ä¸å®Œå…¨ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚‚ä¿æŒ
    pad_last=True         # ä¸è¶³åˆ†ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
)
# å‡ºåŠ›: windowed_data (num_windows, 150, channels)
#       windowed_labels (num_windows,)
```

- å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ãƒ©ãƒ™ãƒ«ï¼šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®**æœ€é »å€¤**
- æœ€å¾Œã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼š`edge`ãƒ¢ãƒ¼ãƒ‰ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°

#### 3. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆextract_featureså†…ï¼‰

```python
# ACCãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿ã€scale_factorãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å ´åˆ
if modality_name == 'ACC' and self.scale_factor is not None:
    modality_data = modality_data / self.scale_factor
    logger.info(f"Applied scale_factor={self.scale_factor} to {sensor_name}/{modality_name}")
```

**é‡è¦**:
- **ACCã®ã¿**ã«é©ç”¨ï¼ˆGYROã€MAGã€ECGãªã©ã«ã¯é©ç”¨ã—ãªã„ï¼‰
- ç›®çš„ï¼šç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§åŠ é€Ÿåº¦ã‚’**Gå˜ä½ã«çµ±ä¸€**
- m/sÂ²ãƒ‡ãƒ¼ã‚¿ã®å ´åˆï¼š9.8ã§é™¤ç®—ã—ã¦Gå˜ä½ã«å¤‰æ›

#### 4. å½¢çŠ¶å¤‰æ›ã¨ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–

```python
# (num_windows, window_size, channels) -> (num_windows, channels, window_size)
modality_data = np.transpose(modality_data, (0, 2, 1))

# ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚float16ã«å¤‰æ›
modality_data = modality_data.astype(np.float16)
```

- PyTorchã®ç•³ã¿è¾¼ã¿å±¤ã®å…¥åŠ›å½¢å¼ã«åˆã‚ã›ã‚‹ï¼š`(batch, channels, time)`
- float16ï¼šã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨ãƒ¡ãƒ¢ãƒªã‚’ç´„50%å‰Šæ¸›

---

## é‡è¦ãªè¨­è¨ˆåŸå‰‡

### 1. ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿æŒã®åŸå‰‡

**æ­£è¦åŒ–ã¯è¡Œã‚ãªã„** - ãƒ‡ãƒ¼ã‚¿ã¯ç”Ÿã®ã‚»ãƒ³ã‚µãƒ¼å€¤ã®ã¾ã¾ä¿å­˜

```python
# âŒ é–“é•ã„ï¼šæ¨™æº–åŒ–ã‚’é©ç”¨
normalized_data = (data - mean) / std

# âœ… æ­£ã—ã„ï¼šç”Ÿãƒ‡ãƒ¼ã‚¿ã®ã¾ã¾ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ã¿ï¼‰
if modality_name == 'ACC' and self.scale_factor is not None:
    data = data / self.scale_factor  # å˜ä½çµ±ä¸€ã®ã¿
```

**ç†ç”±**:
- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã§å‹•çš„ã«æ­£è¦åŒ–ã™ã‚‹ï¼ˆå®Ÿé¨“ã”ã¨ã«å¤‰æ›´å¯èƒ½ï¼‰
- å‰å‡¦ç†ã§ã®æ­£è¦åŒ–ã¯ä¸å¯é€†ã§æŸ”è»Ÿæ€§ã‚’å¤±ã†
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«`'normalization': 'none'`ã‚’æ˜è¨˜

### 2. å˜ä½çµ±ä¸€ã®åŸå‰‡

**åŠ é€Ÿåº¦ã¯å¿…ãšGå˜ä½ã«çµ±ä¸€**

```python
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒm/sÂ²ã®å ´åˆ
"scale_factor": 9.8  # dataset_info.pyã§å®šç¾©

# preprocessorå†…ã§é©ç”¨
if modality_name == 'ACC' and self.scale_factor is not None:
    modality_data = modality_data / self.scale_factor
```

**ç†ç”±**:
- ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§ã®å€¤ã®ç¯„å›²ã‚’çµ±ä¸€
- ãƒ¢ãƒ‡ãƒ«ãŒè¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã™ã‚‹éš›ã®å®‰å®šæ€§å‘ä¸Š

### 3. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéšå±¤ã®åŸå‰‡

**å¿…ãšä»¥ä¸‹ã®æ§‹é€ ã‚’å®ˆã‚‹**:

```
data/processed/{dataset_name}/
â”œâ”€â”€ USER00001/
â”‚   â”œâ”€â”€ {Sensor1}/
â”‚   â”‚   â”œâ”€â”€ {Modality1}/
â”‚   â”‚   â”‚   â”œâ”€â”€ X.npy  # (num_windows, channels, window_size) - float16
â”‚   â”‚   â”‚   â””â”€â”€ Y.npy  # (num_windows,) - int
â”‚   â”‚   â”œâ”€â”€ {Modality2}/
â”‚   â”‚   â”‚   â”œâ”€â”€ X.npy
â”‚   â”‚   â”‚   â””â”€â”€ Y.npy
â”‚   â”œâ”€â”€ {Sensor2}/
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ USER00002/
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.json
```

**é‡è¦**:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: `USER00001`å½¢å¼ï¼ˆ5æ¡ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
- ã‚»ãƒ³ã‚µãƒ¼/ãƒ¢ãƒ€ãƒªãƒ†ã‚£: ãƒ‘ã‚¹åŒºåˆ‡ã‚Šï¼ˆä¾‹ï¼š`Torso/ACC`ï¼‰
- ãƒ•ã‚¡ã‚¤ãƒ«å: å¿…ãš`X.npy`ã¨`Y.npy`

### 4. ãƒ©ãƒ™ãƒ«å‡¦ç†ã®åŸå‰‡

**æœªå®šç¾©ã‚¯ãƒ©ã‚¹ã¯-1ã§çµ±ä¸€**

```python
# ãƒ©ãƒ™ãƒ«å¤‰æ›ä¾‹ï¼ˆMHEALTHã®å ´åˆï¼‰
# å…ƒã®ãƒ©ãƒ™ãƒ«: 0ï¼ˆç„¡æ´»å‹•ï¼‰ã€1-12ï¼ˆæœ‰åŠ¹ãªã‚¯ãƒ©ã‚¹ï¼‰
labels = np.where(labels == 0, -1, labels - 1)
# çµæœ: -1ï¼ˆæœªå®šç¾©ï¼‰ã€0-11ï¼ˆæœ‰åŠ¹ãªã‚¯ãƒ©ã‚¹ï¼‰
```

**ç†ç”±**:
- è¨“ç·´æ™‚ã«æœªå®šç¾©ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯èƒ½
- ã‚¯ãƒ©ã‚¹æ•°ã®ä¸€è²«æ€§ã‚’ä¿ã¤ï¼ˆn_classes=12ã®å ´åˆã€0-11ï¼‰

### 5. ãƒ­ã‚°å‡ºåŠ›ã®åŸå‰‡

**å‡¦ç†ã®å„æ®µéšã§è©³ç´°ãªãƒ­ã‚°ã‚’å‡ºåŠ›**

```python
logger.info(f"USER{person_id:05d}: {data.shape}, Labels: {labels.shape}")
logger.info(f"USER{person_id:05d} cleaned and resampled: {resampled_data.shape}")
logger.info(f"  {sensor_modality_key}: X.shape={X.shape}, Y.shape={Y.shape}")
```

**å«ã‚ã‚‹ã¹ãæƒ…å ±**:
- ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶
- å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®å®Œäº†
- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨ã®æœ‰ç„¡

---

## ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å®Ÿè£…å‰

- [ ] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è«–æ–‡ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç†Ÿèª­
- [ ] ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ç¢ºèª
- [ ] **åŠ é€Ÿåº¦ã‚»ãƒ³ã‚µãƒ¼ã®å˜ä½ã‚’ç¢ºèª**ï¼ˆGã€m/sÂ²ã€mgãªã©ï¼‰
- [ ] ã‚»ãƒ³ã‚µãƒ¼é…ç½®ã¨ãƒãƒ£ãƒ³ãƒãƒ«æ§‹æˆã‚’å›³è§£
- [ ] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’ç¢ºèª
- [ ] ãƒ©ãƒ™ãƒ«ä½“ç³»ã‚’ç†è§£ï¼ˆæœªå®šç¾©ã‚¯ãƒ©ã‚¹ã®æœ‰ç„¡ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ–¹å¼ï¼‰

### å®Ÿè£…ä¸­

- [ ] `dataset_info.py`ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
  - [ ] `scale_factor`ã‚’æ­£ã—ãè¨­å®šï¼ˆm/sÂ²ã®å ´åˆã¯9.8ï¼‰
  - [ ] `has_undefined_class`ã‚’è¨­å®š
  - [ ] `labels`è¾æ›¸ã‚’å®Œå…¨ã«å®šç¾©
- [ ] `preprocessors/your_dataset.py`ã‚’å®Ÿè£…
  - [ ] `@register_preprocessor`ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’å¿˜ã‚Œãšã«
  - [ ] `scale_factor`ã‚’`DATASETS`ã‹ã‚‰èª­ã¿è¾¼ã‚€
  - [ ] ACCãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é©ç”¨
  - [ ] float16ã«å¤‰æ›
  - [ ] å½¢çŠ¶ã‚’`(N, C, T)`ã«å¤‰æ›
- [ ] `configs/preprocess.yaml`ã«è¨­å®šã‚’è¿½åŠ 
- [ ] ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ

### å®Ÿè£…å¾Œ

- [ ] `python preprocess.py --list`ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹
- [ ] å‰å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã™ã‚‹
- [ ] `metadata.json`ã®å†…å®¹ã‚’ç¢ºèª
  - [ ] `normalization: none`ã«ãªã£ã¦ã„ã‚‹ã‹
  - [ ] `scale_factor`ãŒæ­£ã—ãè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹
  - [ ] `data_dtype: float16`ã«ãªã£ã¦ã„ã‚‹ã‹
- [ ] ç”Ÿæˆã•ã‚ŒãŸX.npyã®å½¢çŠ¶ã‚’ç¢ºèªï¼š`(N, C, 150)`
- [ ] ç”Ÿæˆã•ã‚ŒãŸY.npyã®å½¢çŠ¶ã‚’ç¢ºèªï¼š`(N,)`
- [ ] ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’ç¢ºèªï¼ˆæ¥µç«¯ãªä¸å‡è¡¡ãŒãªã„ã‹ï¼‰
- [ ] ãƒ‡ãƒ¼ã‚¿ã®å€¤ã®ç¯„å›²ã‚’ç¢ºèª
  - [ ] ACCãŒé©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã‹ï¼ˆ-10G ~ +10Gç¨‹åº¦ï¼‰
  - [ ] NaN/InfãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹
- [ ] å¯è¦–åŒ–ã‚µãƒ¼ãƒãƒ¼ã§å‹•ä½œç¢ºèªï¼š`python visualize_server.py`
- [ ] ãƒ†ã‚¹ãƒˆãŒã™ã¹ã¦ãƒ‘ã‚¹ã™ã‚‹

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [ ] README.mdã®ã‚µãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã‚’æ›´æ–°
- [ ] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®æ³¨æ„äº‹é …ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã«è¨˜è¼‰
- [ ] å®Ÿè£…æ™‚ã®åˆ¤æ–­ï¼ˆãªãœã“ã®æ–¹æ³•ã‚’é¸ã‚“ã ã‹ï¼‰ã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### 1. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒæ­£ã—ãé©ç”¨ã•ã‚Œãªã„

**ç—‡çŠ¶**: ACCã®å€¤ã®ç¯„å›²ãŒä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨å¤§ããç•°ãªã‚‹

**åŸå› **:
- `scale_factor`ãŒæœªè¨­å®šã¾ãŸã¯é–“é•ã£ãŸå€¤
- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ¡ä»¶åˆ†å²ãŒæ­£ã—ããªã„

**è§£æ±ºç­–**:
```python
# dataset_info.pyã§å¿…ãšå®šç¾©
"scale_factor": 9.8  # m/sÂ²ã®å ´åˆ

# preprocessorå†…ã§å¿…ãšé©ç”¨
if modality_name == 'ACC' and self.scale_factor is not None:
    modality_data = modality_data / self.scale_factor
    logger.info(f"Applied scale_factor to {sensor_name}/{modality_name}")  # ãƒ­ã‚°ç¢ºèª
```

#### 2. ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œã«ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒåˆã‚ãªã„

**ç—‡çŠ¶**: ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œã®dataã¨labelsã®é•·ã•ãŒç•°ãªã‚‹

**åŸå› **: `resample_timeseries`ã®ãƒ©ãƒ™ãƒ«è£œé–“ãŒä¸é©åˆ‡

**è§£æ±ºç­–**: `utils.py`ã®å®Ÿè£…ã‚’ä½¿ç”¨ï¼ˆæœ€è¿‘å‚è£œé–“ã‚’ä½¿ç”¨ï¼‰

#### 3. ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åŒ–ã§ãƒ©ãƒ™ãƒ«ãŒ-1ã«ãªã‚‹

**ç—‡çŠ¶**: æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãªã®ã«windowed_labelsãŒ-1

**åŸå› **: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã«-1ãŒæœ€é »å€¤ã¨ã—ã¦é¸ã°ã‚ŒãŸ

**è§£æ±ºç­–**:
- ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º/ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã‚’èª¿æ•´
- ã¾ãŸã¯ã€-1ã‚’å«ã‚€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å¾Œå‡¦ç†ã§é™¤å¤–

#### 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: `TypeError: Object of type int64 is not JSON serializable`

**åŸå› **: NumPyå‹ãŒJSONéå¯¾å¿œ

**è§£æ±ºç­–**: `save_processed_data`ã®å¤‰æ›é–¢æ•°ã‚’ä½¿ç”¨ï¼ˆdsads.pyã‚’å‚ç…§ï¼‰

---

## å‚è€ƒå®Ÿè£…

æ—¢å­˜ã®å®Ÿè£…ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š

- **ã‚·ãƒ³ãƒ—ãƒ«ãªä¾‹**: `src/preprocessors/dsads.py`
  - å…¨ã‚»ãƒ³ã‚µãƒ¼ãŒåŒã˜ãƒ¢ãƒ€ãƒªãƒ†ã‚£æ§‹æˆ
  - æ¨™æº–çš„ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åŒ–ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

- **è¤‡é›‘ãªä¾‹**: `src/preprocessors/mhealth.py`
  - ã‚»ãƒ³ã‚µãƒ¼ã”ã¨ã«ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£
  - æœªå®šç¾©ã‚¯ãƒ©ã‚¹ï¼ˆ-1ï¼‰ã®å‡¦ç†
  - ãƒ©ãƒ™ãƒ«å¤‰æ›ï¼ˆ0â†’-1ã€1-12â†’0-11ï¼‰

- **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè£…**: `src/preprocessors/dsads.py`ã®`download_dataset()`
  - UCI Machine Learning Repositoryã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
  - è§£å‡ã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•´ç†

---

## ã¾ã¨ã‚

æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã™ã‚‹éš›ã®**æœ€é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:

1. **åŠ é€Ÿåº¦ã®å˜ä½ã‚’ç¢ºèªã—ã€scale_factorã‚’æ­£ã—ãè¨­å®šã™ã‚‹**
2. **ACCãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹**
3. **æ­£è¦åŒ–ã¯è¡Œã‚ãšã€ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹**
4. **ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã®è¦ç´„ã‚’å®ˆã‚‹**
5. **æœªå®šç¾©ã‚¯ãƒ©ã‚¹ã¯-1ã§çµ±ä¸€ã™ã‚‹**
6. **float16ã¨ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶(N, C, T)ã‚’å®ˆã‚‹**

ã“ã‚Œã‚‰ã®åŸå‰‡ã‚’å®ˆã‚‹ã“ã¨ã§ã€è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§ã®ä¸€è²«æ€§ãŒä¿ãŸã‚Œã€har-foundationãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®å­¦ç¿’ãŒå††æ»‘ã«è¡Œãˆã¾ã™ã€‚

ç–‘å•ç‚¹ãŒã‚ã‚Œã°ã€æ—¢å­˜ã®å®Ÿè£…ï¼ˆdsads.pyã€mhealth.pyï¼‰ã‚’å‚ç…§ã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¾é ¼ã—ã¦ãã ã•ã„ã€‚

---

## AIå®Ÿè£…æ™‚ã®æ³¨æ„äº‹é …

AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆClaude Codeç­‰ï¼‰ãŒã“ã®ã‚¬ã‚¤ãƒ‰ã‚’ä½¿ç”¨ã—ã¦æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã™ã‚‹å ´åˆã€ä»¥ä¸‹ã®ç‚¹ã«ç‰¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

### ğŸ¤– å¿…é ˆã®å®Ÿè£…æ‰‹é †

#### 1. **æ—¢å­˜å®Ÿè£…ã‚’å¿…ãšèª­ã‚€**

å®Ÿè£…ã‚’é–‹å§‹ã™ã‚‹å‰ã«ã€**å¿…ãšä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚“ã§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç†è§£**ã—ã¦ãã ã•ã„ï¼š

```bash
# å¿…èª­ãƒ•ã‚¡ã‚¤ãƒ«
src/dataset_info.py              # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å®šç¾©æ–¹æ³•
src/preprocessors/base.py        # åŸºåº•ã‚¯ãƒ©ã‚¹ã®æ§‹é€ 
src/preprocessors/dsads.py       # ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…ä¾‹
src/preprocessors/mhealth.py     # è¤‡é›‘ãªå®Ÿè£…ä¾‹
src/preprocessors/utils.py       # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
configs/preprocess.yaml          # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼
```

**æ¨æ¸¬ã‚„å‰µä½œã¯ç¦æ­¢** - å¿…ãšæ—¢å­˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã£ã¦ãã ã•ã„ã€‚

#### 2. **ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆã‹ã‚‰å§‹ã‚ã‚‹**

æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®Ÿè£…ã¯ã€**æœ€ã‚‚ä¼¼ã¦ã„ã‚‹æ—¢å­˜å®Ÿè£…ã‚’ã‚³ãƒ”ãƒ¼**ã—ã¦ã‹ã‚‰ä¿®æ­£ã—ã¦ãã ã•ã„ï¼š

- å…¨ã‚»ãƒ³ã‚µãƒ¼ãŒåŒã˜ãƒ¢ãƒ€ãƒªãƒ†ã‚£ â†’ `dsads.py`ã‚’ã‚³ãƒ”ãƒ¼
- ã‚»ãƒ³ã‚µãƒ¼ã”ã¨ã«ãƒ¢ãƒ€ãƒªãƒ†ã‚£ãŒç•°ãªã‚‹ â†’ `mhealth.py`ã‚’ã‚³ãƒ”ãƒ¼

```bash
# ä¾‹: DSADSãƒ™ãƒ¼ã‚¹ã§æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
cp src/preprocessors/dsads.py src/preprocessors/your_dataset.py
# ãã®å¾Œã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®éƒ¨åˆ†ã®ã¿ä¿®æ­£
```

#### 3. **æ®µéšçš„ã«å®Ÿè£…ãƒ»ãƒ†ã‚¹ãƒˆã™ã‚‹**

ä¸€åº¦ã«ã™ã¹ã¦ã‚’å®Ÿè£…ã›ãšã€ä»¥ä¸‹ã®é †åºã§æ®µéšçš„ã«é€²ã‚ã¦ãã ã•ã„ï¼š

```python
# ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç™»éŒ²ã®ã¿
# dataset_info.pyã«è¿½åŠ  â†’ python preprocess.py --list ã§ç¢ºèª

# ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¯ãƒ©ã‚¹ã®éª¨çµ„ã¿
# get_dataset_name()ã®ã¿å®Ÿè£… â†’ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã§ãã‚‹ã‹ç¢ºèª

# ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# load_raw_data()ã‚’å®Ÿè£… â†’ 1ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚ã‚‹ã‹ç¢ºèª

# ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
# clean_data()ã‚’å®Ÿè£… â†’ ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹ç¢ºèª

# ã‚¹ãƒ†ãƒƒãƒ—5: ç‰¹å¾´æŠ½å‡º
# extract_features()ã‚’å®Ÿè£… â†’ å½¢çŠ¶ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ç¢ºèª

# ã‚¹ãƒ†ãƒƒãƒ—6: ä¿å­˜
# save_processed_data()ã‚’å®Ÿè£… â†’ ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãä¿å­˜ã•ã‚Œã‚‹ã‹ç¢ºèª
```

å„ã‚¹ãƒ†ãƒƒãƒ—ã§å‹•ä½œç¢ºèªã—ã¦ã‹ã‚‰æ¬¡ã«é€²ã‚“ã§ãã ã•ã„ã€‚

#### 4. **çµ¶å¯¾ã«å¤‰ãˆã¦ã¯ã„ã‘ãªã„ã‚‚ã®**

ä»¥ä¸‹ã®è¦ç´ ã¯**æ—¢å­˜å®Ÿè£…ã¨å®Œå…¨ã«ä¸€è‡´**ã•ã›ã¦ãã ã•ã„ï¼š

##### ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶
```python
# âœ… å¿…ãšã“ã®å½¢çŠ¶
X.shape = (num_windows, channels, window_size)  # ä¾‹: (1000, 3, 150)
Y.shape = (num_windows,)                        # ä¾‹: (1000,)

# âŒ ã“ã‚Œã‚‰ã¯é–“é•ã„
X.shape = (num_windows, window_size, channels)  # è»¸ã®é †åºãŒé•ã†
X.shape = (num_windows, channels)                # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åŒ–ã•ã‚Œã¦ã„ãªã„
```

##### ãƒ‡ãƒ¼ã‚¿å‹
```python
# âœ… å¿…ãšfloat16
X = X.astype(np.float16)

# âŒ ã“ã‚Œã‚‰ã¯é–“é•ã„
X = X.astype(np.float32)  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒæ‚ªã„
X = X.astype(np.float64)  # ã•ã‚‰ã«æ‚ªã„
```

##### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
```python
# âœ… å¿…ãšã“ã®æ§‹é€ 
data/processed/{dataset_name}/USER00001/{Sensor}/{Modality}/X.npy
                                                           /Y.npy

# âŒ ã“ã‚Œã‚‰ã¯é–“é•ã„
data/processed/{dataset_name}/user1/sensor1/acc/X.npy     # å‘½åè¦å‰‡é•å
data/processed/{dataset_name}/USER1/Sensor1_ACC/X.npy     # éšå±¤ãŒé•ã†
```

##### ãƒ•ã‚¡ã‚¤ãƒ«å
```python
# âœ… å¿…ãšã“ã®ãƒ•ã‚¡ã‚¤ãƒ«å
X.npy  # å¤§æ–‡å­—ã®X
Y.npy  # å¤§æ–‡å­—ã®Y

# âŒ ã“ã‚Œã‚‰ã¯é–“é•ã„
x.npy, y.npy           # å°æ–‡å­—ã¯ä¸å¯
data.npy, labels.npy   # åˆ¥ã®åå‰ã¯ä¸å¯
```

#### 5. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**

ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯æœ€ã‚‚é–“é•ã„ã‚„ã™ã„éƒ¨åˆ†ã§ã™ã€‚ä»¥ä¸‹ã‚’å³å¯†ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼š

```python
# ãƒã‚§ãƒƒã‚¯1: dataset_info.pyã§scale_factorãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹
DATASETS = {
    "YOUR_DATASET": {
        "scale_factor": 9.8,  # m/sÂ²ã®å ´åˆã®ã¿è¨­å®š
        # ...
    }
}

# ãƒã‚§ãƒƒã‚¯2: Preprocessorã§èª­ã¿è¾¼ã‚“ã§ã„ã‚‹ã‹
self.scale_factor = DATASETS.get('YOUR_DATASET', {}).get('scale_factor', None)

# ãƒã‚§ãƒƒã‚¯3: extract_featureså†…ã§æ­£ã—ãé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹
if modality_name == 'ACC' and self.scale_factor is not None:
    modality_data = modality_data / self.scale_factor
    logger.info(f"  Applied scale_factor={self.scale_factor} to {sensor_name}/{modality_name}")

# ãƒã‚§ãƒƒã‚¯4: ãƒ­ã‚°ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
# ãƒ­ã‚°ã« "Applied scale_factor=9.8 to Torso/ACC" ã®ã‚ˆã†ãªè¡ŒãŒå‡ºåŠ›ã•ã‚Œã‚‹
```

#### 6. **ã‚ˆãã‚ã‚‹é–“é•ã„ã¨ä¿®æ­£æ–¹æ³•**

##### é–“é•ã„1: ãƒ¢ãƒ€ãƒªãƒ†ã‚£åã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
```python
# âŒ é–“é•ã„
if modality_name == 'acc':  # å°æ–‡å­—

# âœ… æ­£ã—ã„
if modality_name == 'ACC':  # å¤§æ–‡å­—
```

##### é–“é•ã„2: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ã™ã¹ã¦ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«é©ç”¨
```python
# âŒ é–“é•ã„
modality_data = modality_data / self.scale_factor  # æ¡ä»¶ãªã—

# âœ… æ­£ã—ã„
if modality_name == 'ACC' and self.scale_factor is not None:
    modality_data = modality_data / self.scale_factor
```

##### é–“é•ã„3: å½¢çŠ¶å¤‰æ›ã®å¿˜ã‚Œ
```python
# âŒ é–“é•ã„
# (N, T, C)ã®ã¾ã¾ä¿å­˜

# âœ… æ­£ã—ã„
modality_data = np.transpose(modality_data, (0, 2, 1))  # (N, C, T)ã«å¤‰æ›
```

##### é–“é•ã„4: float16å¤‰æ›ã®å¿˜ã‚Œ
```python
# âŒ é–“é•ã„
# float64ã‚„float32ã®ã¾ã¾ä¿å­˜

# âœ… æ­£ã—ã„
modality_data = modality_data.astype(np.float16)
```

##### é–“é•ã„5: ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã®å¿˜ã‚Œ
```python
# âŒ é–“é•ã„
class YourDatasetPreprocessor(BasePreprocessor):

# âœ… æ­£ã—ã„
@register_preprocessor('your_dataset')  # ã“ã‚Œã‚’å¿˜ã‚Œãšã«ï¼
class YourDatasetPreprocessor(BasePreprocessor):
```

#### 7. **å®Ÿè£…å®Œäº†å¾Œã®æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**

å®Ÿè£…ãŒå®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§æ¤œè¨¼ã—ã¦ãã ã•ã„ï¼š

```python
import numpy as np
from pathlib import Path

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’æŒ‡å®š
DATASET_NAME = "your_dataset"
USER_ID = "USER00001"
SENSOR = "Sensor1"
MODALITY = "ACC"

# ãƒ‘ã‚¹æ§‹ç¯‰
base_path = Path(f"data/processed/{DATASET_NAME}/{USER_ID}/{SENSOR}/{MODALITY}")
X_path = base_path / "X.npy"
Y_path = base_path / "Y.npy"

# ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
assert X_path.exists(), f"X.npy not found: {X_path}"
assert Y_path.exists(), f"Y.npy not found: {Y_path}"
print("âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã™")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
X = np.load(X_path)
Y = np.load(Y_path)

# å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
assert X.ndim == 3, f"X should be 3D, got {X.ndim}D"
assert Y.ndim == 1, f"Y should be 1D, got {Y.ndim}D"
assert X.shape[0] == Y.shape[0], f"Sample count mismatch: X={X.shape[0]}, Y={Y.shape[0]}"
assert X.shape[2] == 150, f"Window size should be 150, got {X.shape[2]}"
print(f"âœ“ å½¢çŠ¶ãŒæ­£ã—ã„: X{X.shape}, Y{Y.shape}")

# ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
assert X.dtype == np.float16, f"X should be float16, got {X.dtype}"
print(f"âœ“ ãƒ‡ãƒ¼ã‚¿å‹ãŒæ­£ã—ã„: {X.dtype}")

# å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆACCï¼‰
if MODALITY == "ACC":
    assert X.min() > -20, f"ACCå€¤ãŒç•°å¸¸ã«å°ã•ã„: {X.min()}"
    assert X.max() < 20, f"ACCå€¤ãŒç•°å¸¸ã«å¤§ãã„: {X.max()}"
    print(f"âœ“ ACCå€¤ã®ç¯„å›²ãŒå¦¥å½“: [{X.min():.2f}, {X.max():.2f}]")

# NaN/Infãƒã‚§ãƒƒã‚¯
assert not np.isnan(X).any(), "X contains NaN"
assert not np.isinf(X).any(), "X contains Inf"
print("âœ“ NaN/InfãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

# ãƒ©ãƒ™ãƒ«ãƒã‚§ãƒƒã‚¯
unique_labels = np.unique(Y)
print(f"âœ“ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ©ãƒ™ãƒ«: {unique_labels}")

print("\n=== ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹ã—ã¾ã—ãŸ ===")
```

#### 8. **ãƒ‡ãƒãƒƒã‚°ã®ãƒ’ãƒ³ãƒˆ**

å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆï¼š

1. **ãƒ­ã‚°ã‚’ç¢ºèª**
   ```bash
   tail -n 100 logs/preprocessing/preprocess.log
   ```

2. **ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª**
   ```python
   # load_raw_data()ã®ç›´å¾Œã«è¿½åŠ 
   print(f"Loaded data shape: {data.shape}")
   print(f"Loaded labels shape: {labels.shape}")
   print(f"Labels unique: {np.unique(labels)}")
   ```

3. **æ—¢å­˜å®Ÿè£…ã¨æ¯”è¼ƒ**
   ```bash
   # DSADSã®å‡¦ç†çµæœã¨æ¯”è¼ƒ
   ls -lh data/processed/dsads/USER00001/Torso/ACC/
   ls -lh data/processed/your_dataset/USER00001/Sensor1/ACC/

   # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ããç•°ãªã‚‹å ´åˆã€ä½•ã‹ãŒãŠã‹ã—ã„
   ```

4. **1ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿ã§è©¦ã™**
   ```python
   # load_raw_data()å†…ã§1ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿å‡¦ç†
   for subject_id in range(1, 2):  # 1ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿
       # ...
   ```

### ğŸ¯ å®Ÿè£…ã®æˆåŠŸåŸºæº–

ä»¥ä¸‹ãŒã™ã¹ã¦æº€ãŸã•ã‚Œã¦ã„ã‚Œã°å®Ÿè£…æˆåŠŸã§ã™ï¼š

- [ ] `python preprocess.py --list`ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹
- [ ] `python preprocess.py --dataset your_dataset`ãŒã‚¨ãƒ©ãƒ¼ãªãå®Œäº†ã™ã‚‹
- [ ] `data/processed/your_dataset/`ä»¥ä¸‹ã«æ­£ã—ã„éšå±¤ã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã‚‹
- [ ] ã™ã¹ã¦ã®`X.npy`ãŒ`(N, C, 150)`ã®å½¢çŠ¶
- [ ] ã™ã¹ã¦ã®`Y.npy`ãŒ`(N,)`ã®å½¢çŠ¶
- [ ] ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒ`float16`å‹
- [ ] `metadata.json`ãŒç”Ÿæˆã•ã‚Œã€æ­£ã—ã„å†…å®¹ãŒå«ã¾ã‚Œã‚‹
- [ ] ACCã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒæ­£ã—ãé©ç”¨ã•ã‚Œã¦ã„ã‚‹ï¼ˆãƒ­ã‚°ã§ç¢ºèªï¼‰
- [ ] æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã™ã¹ã¦ãƒ‘ã‚¹ã™ã‚‹
- [ ] å¯è¦–åŒ–ã‚µãƒ¼ãƒãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ãŒè¡¨ç¤ºã•ã‚Œã‚‹

### ğŸ“ å®Ÿè£…æ™‚ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³

AIãŒå®Ÿè£…ã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’æ˜ç¤ºçš„ã«å ±å‘Šã—ã¦ãã ã•ã„ï¼š

1. **å®Ÿè£…é–‹å§‹æ™‚**
   - ã©ã®æ—¢å­˜å®Ÿè£…ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹ã‹
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸»è¦ãªç‰¹å¾´ï¼ˆã‚»ãƒ³ã‚µãƒ¼æ•°ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼‰
   - scale_factorã®å€¤ã¨ãã®ç†ç”±

2. **å®Ÿè£…ä¸­**
   - å„ã‚¹ãƒ†ãƒƒãƒ—ã®å®Œäº†å ±å‘Š
   - æ—¢å­˜å®Ÿè£…ã¨ç•°ãªã‚‹éƒ¨åˆ†ã®èª¬æ˜
   - ä¸æ˜ç‚¹ã‚„åˆ¤æ–­ãŒå¿…è¦ãªç®‡æ‰€

3. **å®Ÿè£…å®Œäº†æ™‚**
   - æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®çµæœ
   - ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆæƒ…å ±
   - æ—¢çŸ¥ã®åˆ¶é™äº‹é …ã‚„æ³¨æ„ç‚¹

ã“ã‚Œã«ã‚ˆã‚Šã€äººé–“ãŒå®Ÿè£…ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã‚„ã™ããªã‚Šã€å•é¡Œã®æ—©æœŸç™ºè¦‹ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

---

## ç‰¹æ®Šãªã‚±ãƒ¼ã‚¹ï¼šã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ã‚»ãƒ³ã‚µãƒ¼ãƒ»å¯å¤‰ã‚«ãƒ©ãƒ CSV

### ã‚±ãƒ¼ã‚¹: TMDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‹ï¼ˆã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚»ãƒ³ã‚µãƒ¼ï¼‰

ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

#### ç‰¹å¾´
1. **ã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: å›ºå®šãƒ¬ãƒ¼ãƒˆã§ã¯ãªãã€å€¤ãŒå¤‰åŒ–ã—ãŸã¨ãã«ãƒ‡ãƒ¼ã‚¿ãŒè¨˜éŒ²ã•ã‚Œã‚‹
2. **å¯å¤‰ã‚«ãƒ©ãƒ æ•°**: ã‚»ãƒ³ã‚µãƒ¼ã‚¿ã‚¤ãƒ—ã”ã¨ã«ç•°ãªã‚‹ã‚«ãƒ©ãƒ æ•°
   - accelerometer: 5åˆ—ï¼ˆtimestamp, sensor_type, x, y, zï¼‰
   - gyroscope: 5åˆ—ï¼ˆtimestamp, sensor_type, x, y, zï¼‰
   - magnetic_field_uncalibrated: 8åˆ—ï¼ˆtimestamp, sensor_type, x, y, z, bias_x, bias_y, bias_zï¼‰
   - rotation_vector: 7åˆ—ï¼ˆtimestamp, sensor_type, x, y, z, w, accuracyï¼‰
3. **è¤‡æ•°ã‚»ãƒ³ã‚µãƒ¼æ··åœ¨**: 1ã¤ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¤‡æ•°ç¨®é¡ã®ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹

#### å®Ÿè£…ä¸Šã®èª²é¡Œ

**âŒ å•é¡Œ: pandasã®read_csvã¯å›ºå®šã‚«ãƒ©ãƒ æ•°ã‚’æœŸå¾…ã™ã‚‹**

```python
# ã“ã‚Œã¯å¤±æ•—ã™ã‚‹ï¼ˆã‚«ãƒ©ãƒ æ•°ãŒè¡Œã”ã¨ã«ç•°ãªã‚‹ï¼‰
df = pd.read_csv(csv_file, header=None, names=['timestamp', 'sensor_type', 'x', 'y', 'z'])
# Error: Expected 5 fields, saw 8
```

**âœ… è§£æ±ºç­–1: æ‰‹å‹•ãƒ‘ãƒ¼ã‚¹**

```python
def _parse_csv_manual(self, csv_file: Path) -> np.ndarray:
    """
    å¯å¤‰ã‚«ãƒ©ãƒ æ•°ã®CSVã‚’æ‰‹å‹•ã§ãƒ‘ãƒ¼ã‚¹
    """
    acc_data = []
    gyro_data = []

    with open(csv_file, 'r') as f:
        for line in f:
            parts = line.strip().split(',')
            if len(parts) >= 4:
                try:
                    timestamp = float(parts[0])
                    sensor_type = parts[1]
                    x = float(parts[2])
                    y = float(parts[3])
                    z = float(parts[4]) if len(parts) > 4 else 0.0

                    if sensor_type == 'android.sensor.accelerometer':
                        acc_data.append([timestamp, x, y, z])
                    elif sensor_type == 'android.sensor.gyroscope':
                        gyro_data.append([timestamp, x, y, z])
                except ValueError:
                    continue

    # ... ä»¥é™ã€acc_dataã¨gyro_dataã‚’å‡¦ç†
```

**âœ… è§£æ±ºç­–2: pandasã®ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š**

```python
# 'python'ã‚¨ãƒ³ã‚¸ãƒ³ã¯å¯å¤‰ã‚«ãƒ©ãƒ ã«å¯¾å¿œï¼ˆä½é€Ÿï¼‰
df = pd.read_csv(csv_file, header=None, engine='python', on_bad_lines='skip')
```

#### ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—çµ±åˆ

ã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ãƒ‡ãƒ¼ã‚¿ã¯ã€ã‚»ãƒ³ã‚µãƒ¼ã”ã¨ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒç•°ãªã‚‹ãŸã‚ã€çµ±åˆãŒå¿…è¦ã§ã™ï¼š

```python
def _align_sensor_data(self, acc_df: pd.DataFrame, gyro_df: pd.DataFrame) -> np.ndarray:
    """
    åŠ é€Ÿåº¦ã¨ã‚¸ãƒ£ã‚¤ãƒ­ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æƒãˆã¦ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
    """
    # å…±é€šã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç¯„å›²ã‚’å–å¾—
    min_time = max(acc_df['timestamp'].min(), gyro_df['timestamp'].min())
    max_time = min(acc_df['timestamp'].max(), gyro_df['timestamp'].max())

    # å›ºå®šã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆæ¨å®šï¼‰ã§ç­‰é–“éš”ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆ
    estimated_rate = 50  # Hzï¼ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®šï¼‰
    num_samples = int((max_time - min_time) / 1000.0 * estimated_rate)
    resampled_times = np.linspace(min_time, max_time, num_samples)

    # ç·šå½¢è£œé–“ã§å„ã‚»ãƒ³ã‚µãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    acc_resampled = np.zeros((num_samples, 3))
    gyro_resampled = np.zeros((num_samples, 3))

    for i, col in enumerate(['x', 'y', 'z']):
        acc_resampled[:, i] = np.interp(
            resampled_times,
            acc_df['timestamp'].values,
            acc_df[col].values
        )
        gyro_resampled[:, i] = np.interp(
            resampled_times,
            gyro_df['timestamp'].values,
            gyro_df[col].values
        )

    # ACC + GYROã‚’çµåˆ
    combined = np.hstack([acc_resampled, gyro_resampled])
    return combined
```

#### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆæ¨å®š

ã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€`original_sampling_rate`ã¯æ¨å®šå€¤ã§ã™ï¼š

```python
# dataset_info.py
"original_sampling_rate": None,  # å¯å¤‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ï¼‰

# preprocessorå†…ã§æ¨å®šå€¤ã‚’ä½¿ç”¨
estimated_rate = 50  # Hzï¼ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®šã€ã¾ãŸã¯è«–æ–‡ã‹ã‚‰ï¼‰
```

#### ãƒ©ãƒ™ãƒ«æŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ï¼‰

TMDã§ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«åã«ãƒ©ãƒ™ãƒ«æƒ…å ±ãŒå«ã¾ã‚Œã¾ã™ï¼š

```python
# ãƒ•ã‚¡ã‚¤ãƒ«å: sensorfile_U1_Walking_1480512323378.csv
# â†’ Activity: Walking

filename_parts = csv_file.stem.split('_')
if len(filename_parts) >= 3:
    activity_name = filename_parts[2]  # "Walking"
    label = self.activity_map[activity_name]  # 0
```

#### ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ã‚»ãƒ³ã‚µãƒ¼ç”¨ï¼‰

- [ ] å¯å¤‰ã‚«ãƒ©ãƒ æ•°ã«å¯¾å¿œã—ãŸCSVãƒ‘ãƒ¼ã‚¹å®Ÿè£…
- [ ] ã‚»ãƒ³ã‚µãƒ¼ã‚¿ã‚¤ãƒ—ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¿…è¦ãªã‚»ãƒ³ã‚µãƒ¼ã®ã¿æŠ½å‡ºï¼‰
- [ ] ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—çµ±åˆå‡¦ç†ã®å®Ÿè£…
- [ ] ç·šå½¢è£œé–“ã«ã‚ˆã‚‹ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
- [ ] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆæ¨å®šå€¤ã®è¨˜éŒ²
- [ ] ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã®ãƒ©ãƒ™ãƒ«æŠ½å‡ºï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

#### å‚è€ƒå®Ÿè£…

- **TMDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: `src/preprocessors/tmd.py`
  - å¯å¤‰ã‚«ãƒ©ãƒ CSVã®æ‰‹å‹•ãƒ‘ãƒ¼ã‚¹
  - ãƒãƒ«ãƒã‚»ãƒ³ã‚µãƒ¼çµ±åˆ
  - ã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ â†’ å›ºå®šãƒ¬ãƒ¼ãƒˆå¤‰æ›

---

## ğŸš¨ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆå¿…é ˆç¢ºèªäº‹é …ï¼‰

æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¿½åŠ æ™‚ã«ã€**å¿…ãš**ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚éå»ã«é‡å¤§ãªãƒã‚°ãŒç™ºç”Ÿã—ãŸé …ç›®ã§ã™ã€‚

### âœ… 1. æ´»å‹•ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®æ¤œè¨¼ï¼ˆæœ€é‡è¦ï¼‰

**å•é¡Œä¾‹**: SBRHAPTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€activity_labels.txtã®é †åºã¨å®Ÿè£…ãŒå®Œå…¨ã«ä¸ä¸€è‡´

**å¿…é ˆæ‰‹é †**:
```bash
# ã‚¹ãƒ†ãƒƒãƒ—1: å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
cat data/raw/your_dataset/activity_labels.txt
# ã¾ãŸã¯
cat data/raw/your_dataset/README.md

# ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ—ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚µã®å®Ÿè£…ã‚’ç¢ºèª
grep -A 20 "activity_labels\|activity_map" src/preprocessors/your_dataset.py

# ã‚¹ãƒ†ãƒƒãƒ—3: dataset_info.pyã‚’ç¢ºèª
grep -A 15 "YOUR_DATASET" src/dataset_info.py | grep -A 12 "labels"
```

**ãƒã‚§ãƒƒã‚¯é …ç›®**:
- [ ] **å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¿…ãšèª­ã‚€**ï¼ˆREADME, activity_labels.txt, labels.csvç­‰ï¼‰
- [ ] å…ƒãƒ‡ãƒ¼ã‚¿ã® activity_id ã¨å®Ÿè£…ã® activity_labels/activity_map ãŒå®Œå…¨ä¸€è‡´
- [ ] ãƒ—ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚µã¨ dataset_info.py ã® labels ãŒå®Œå…¨ä¸€è‡´
- [ ] 1-indexed â†’ 0-indexed å¤‰æ›ãŒæ­£ã—ãå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- [ ] ãƒ©ãƒ™ãƒ«é †åºã‚’**æ¨æ¸¬ã—ãªã„**ï¼ˆå¿…ãšå…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç¢ºèªï¼‰

**æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰**:
```python
# ç°¡å˜ãªæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
from src.preprocessors import get_preprocessor
from src.dataset_info import DATASETS

config = {'target_sampling_rate': 30, 'window_size': 150, 'stride': 30}
preprocessor = get_preprocessor('your_dataset')(config)

# ãƒ—ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚µã®ãƒ©ãƒ™ãƒ«
print("Preprocessor labels:", preprocessor.activity_labels)

# dataset_info.pyã®ãƒ©ãƒ™ãƒ«
print("Dataset info labels:", DATASETS['YOUR_DATASET']['labels'])

# å¿…ãšä¸€è‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼
```

---

### âœ… 2. ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒ«ã®æ¤œè¨¼ï¼ˆæœ€é‡è¦ï¼‰

**å•é¡Œä¾‹**: IMSBã€IMWSHAã§åŠ é€Ÿåº¦ãŒç•°å¸¸ã«å¤§ãã„ï¼ˆå¹³å‡7Gç­‰ï¼‰

**å¿…é ˆæ‰‹é †**:
```python
# ã‚¹ãƒ†ãƒƒãƒ—1: ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ç¢ºèª
import pandas as pd
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
df = pd.read_csv('data/raw/your_dataset/sample.csv')

# åŠ é€Ÿåº¦ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’ç¢ºèª
print(df[['acc_x', 'acc_y', 'acc_z']].describe())
print("Min/Max:", df[['acc_x', 'acc_y', 'acc_z']].min().min(), 
      df[['acc_x', 'acc_y', 'acc_z']].max().max())
print("Mean:", df[['acc_x', 'acc_y', 'acc_z']].mean())
```

**ãƒã‚§ãƒƒã‚¯é …ç›®**:
- [ ] **å…ƒãƒ‡ãƒ¼ã‚¿ã®README/è«–æ–‡ã§ã‚»ãƒ³ã‚µãƒ¼å˜ä½ã‚’ç¢ºèª**
  - Gå˜ä½: `scale_factor = None`
  - m/sÂ²å˜ä½: `scale_factor = 9.8`
  - mgå˜ä½: `scale_factor = 0.001 * 9.8`ï¼ˆã¾ãŸã¯1000ã§é™¤ç®—å¾Œã«9.8ï¼‰
- [ ] åŠ é€Ÿåº¦ã®å€¤ãŒåˆç†çš„ãªç¯„å›²å†…ã‹ï¼ˆé€šå¸¸Â±2-4Gã€æ¿€ã—ã„å‹•ãã§Â±8Gç¨‹åº¦ï¼‰
- [ ] é‡åŠ›æˆåˆ†ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼ˆé™æ­¢æ™‚ã«1è»¸ãŒç´„Â±1Gï¼‰
- [ ] ã‚¸ãƒ£ã‚¤ãƒ­ã®å˜ä½ï¼ˆé€šå¸¸rad/s ã¾ãŸã¯ deg/sï¼‰
- [ ] `scale_factor`ã¯**ACCãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿**ã«é©ç”¨ã•ã‚Œã‚‹

**æ­£å¸¸ãªå€¤ã®ç›®å®‰**:
```
åŠ é€Ÿåº¦ï¼ˆGå˜ä½ï¼‰:
- é™æ­¢çŠ¶æ…‹: é‡åŠ›æ–¹å‘ã®è»¸ãŒç´„Â±1Gã€ä»–ã®è»¸ã¯ç´„0G
- æ­©è¡Œ: Â±2-3G
- ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°: Â±4-6G
- ã‚¸ãƒ£ãƒ³ãƒ—: Â±8Gç¨‹åº¦

ã‚¸ãƒ£ã‚¤ãƒ­ï¼ˆrad/sï¼‰:
- é™æ­¢çŠ¶æ…‹: ç´„0 rad/s
- é€šå¸¸ã®å‹•ä½œ: Â±1-3 rad/s
- é€Ÿã„å›è»¢: Â±5 rad/sç¨‹åº¦
```

**ç•°å¸¸å€¤ã®ä¾‹**:
```python
# âŒ ç•°å¸¸: å¹³å‡ãŒ7Gï¼ˆé‡åŠ›æ–¹å‘ä»¥å¤–ã‚‚å¤§ãã„ï¼‰
mean: wx=7.4G, wy=7.8G, wz=3.0G
# â†’ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒé–“é•ã£ã¦ã„ã‚‹ã€ã¾ãŸã¯å˜ä½ãŒä¸æ˜

# âœ… æ­£å¸¸: é™æ­¢æ™‚ã«1è»¸ãŒç´„1G
mean: x=0.1G, y=9.8G, z=0.2G  # å…ƒãƒ‡ãƒ¼ã‚¿ãŒm/sÂ²å˜ä½
# â†’ scale_factor=9.8 ãŒå¿…è¦
```

---

### âœ… 3. ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œè¨¼

**å•é¡Œä¾‹**: IMWSHAã§ã€Œ3-imu-one subject.csvã€ã—ã‹æ¢ã•ãšã€Subject 2ä»¥é™ã‚’ç„¡è¦–

**å¿…é ˆæ‰‹é †**:
```bash
# ã‚¹ãƒ†ãƒƒãƒ—1: å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèª
find data/raw/your_dataset -name "*.csv" | head -20

# ã‚¹ãƒ†ãƒƒãƒ—2: å„è¢«é¨“è€…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç¢ºèª
for dir in data/raw/your_dataset/Subject*; do
    echo "=== $dir ==="; ls "$dir"/*.csv
done
```

**ãƒã‚§ãƒƒã‚¯é …ç›®**:
- [ ] å…¨è¢«é¨“è€…ã®ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ãŒä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
- [ ] ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ã—ãªã„
- [ ] `glob('*.csv')` ã‚„ `glob('3-imu*.csv')` ã§ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
- [ ] è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãŒã‚ã‚‹å ´åˆã¯å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ

---

### âœ… 4. è¢«é¨“è€…æ•°ã®æ¤œè¨¼

**å¿…é ˆæ‰‹é †**:
```bash
# å‡¦ç†å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã‚’ç¢ºèª
ls data/processed/your_dataset/ | grep USER | wc -l

# æœŸå¾…ã•ã‚Œã‚‹è¢«é¨“è€…æ•°ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
cat configs/preprocess.yaml | grep -A 5 "your_dataset:" | grep num_subjects
```

**ãƒã‚§ãƒƒã‚¯é …ç›®**:
- [ ] å‡¦ç†å¾Œã®USERæ•°ãŒã€æœŸå¾…ã•ã‚Œã‚‹è¢«é¨“è€…æ•°ã¨ä¸€è‡´
- [ ] metadata.jsonã®usersã‚­ãƒ¼ã«å…¨è¢«é¨“è€…ãŒå«ã¾ã‚Œã‚‹
- [ ] ãƒ­ã‚°ã§ã€ŒLoaded N subjects successfullyã€ã‚’ç¢ºèª

---

### âœ… 5. ã‚¯ãƒ©ã‚¹æ•°ã®æ•´åˆæ€§

**å¿…é ˆæ‰‹é †**:
```python
# dataset_info.pyã®n_classes ã¨ labels ã®æ•´åˆæ€§ã‚’ç¢ºèª
from src.dataset_info import DATASETS

dataset = DATASETS['YOUR_DATASET']
n_classes = dataset['n_classes']
labels = dataset['labels']

# Undefinedã‚¯ãƒ©ã‚¹(-1)ã‚’é™¤å¤–ã—ãŸå®šç¾©æ¸ˆã¿ã‚¯ãƒ©ã‚¹æ•°
defined_classes = [k for k in labels.keys() if k >= 0]

print(f"n_classes: {n_classes}")
print(f"Defined classes: {len(defined_classes)}")
print(f"Labels: {sorted(defined_classes)}")

# n_classes == len(defined_classes) ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼
assert n_classes == len(defined_classes), "n_classes mismatch!"
```

**ãƒã‚§ãƒƒã‚¯é …ç›®**:
- [ ] `n_classes` = å®šç¾©æ¸ˆã¿ã‚¯ãƒ©ã‚¹æ•°ï¼ˆUndefinedé™¤ãï¼‰
- [ ] `has_undefined_class=True` ã®å ´åˆã€-1ãƒ©ãƒ™ãƒ«ãŒ`labels`ã«å­˜åœ¨
- [ ] ãƒ©ãƒ™ãƒ«ãŒ0ã‹ã‚‰é€£ç•ªã§ã‚ã‚‹ã“ã¨ï¼ˆ0, 1, 2, ..., n_classes-1ï¼‰

---

## å®Ÿè£…å®Œäº†å¾Œã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯

**å…¨ã¦ã®æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä»¥ä¸‹ã‚’å®Ÿè¡Œ**:

```bash
# 1. ãƒ—ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚µãŒèª­ã¿è¾¼ã‚ã‚‹ã‹
python -c "from src.preprocessors import get_preprocessor; \
    p = get_preprocessor('your_dataset')({'target_sampling_rate': 30}); \
    print('âœ“ Preprocessor loads successfully')"

# 2. ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®æ¤œè¨¼
python << EOF
from src.preprocessors import get_preprocessor
from src.dataset_info import DATASETS

config = {'target_sampling_rate': 30, 'window_size': 150, 'stride': 30}
preprocessor = get_preprocessor('your_dataset')(config)

# ãƒ—ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚µã®ãƒ©ãƒ™ãƒ«
p_labels = preprocessor.activity_labels  # ã¾ãŸã¯ activity_map
print("Preprocessor labels:", p_labels)

# dataset_info.pyã®ãƒ©ãƒ™ãƒ«
d_labels = DATASETS['YOUR_DATASET']['labels']
print("Dataset info labels:", d_labels)

# ä¸€è‡´ç¢ºèª
if hasattr(preprocessor, 'activity_labels'):
    # 1-indexed â†’ 0-indexed ãƒãƒƒãƒ”ãƒ³ã‚°ã®å ´åˆ
    for original_id, new_id in p_labels.items():
        assert d_labels[new_id] is not None, f"Label {new_id} not in dataset_info"
    print("âœ“ Labels match!")
elif hasattr(preprocessor, 'activity_map'):
    # æ–‡å­—åˆ— â†’ 0-indexed ãƒãƒƒãƒ”ãƒ³ã‚°ã®å ´åˆ
    for activity_str, new_id in p_labels.items():
        assert d_labels[new_id] is not None, f"Label {new_id} not in dataset_info"
    print("âœ“ Labels match!")
